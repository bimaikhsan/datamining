{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Selamat Datang Di Halaman Tugas Penambangan Data \u00b6 Nama : Bima Zainudin Ikhsan Nim : 180411100102 Kelas : Penambangan Data-5B Jurusan : Teknik Informatika Angkatan : 2018 Perguruan Tinggi : Universitas Trunojoyo Madura","title":"biodata"},{"location":"#selamat-datang-di-halaman-tugas-penambangan-data","text":"Nama : Bima Zainudin Ikhsan Nim : 180411100102 Kelas : Penambangan Data-5B Jurusan : Teknik Informatika Angkatan : 2018 Perguruan Tinggi : Universitas Trunojoyo Madura","title":"Selamat Datang Di Halaman Tugas Penambangan Data"},{"location":"Mengukur Jarak/","text":"Pengertian \u00b6 Salah satu tantangan dalam era ini dengan datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Alogritma seperit Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkan. Mengukur Jarak Menggunakan Minkowski Distance \u00b6 Langkah-langkah mengukur jarak kali ini saya menggunakan Minkowski Distance . Minkowski Distance adalah matrik dalam ruang vektor normed yang dapat dianggap sebagai generalisasi dari Euclidean Distance dan Manhattan Distance. $$ d _ { \\operatorname { min } } = ( \\ sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 $$ diman mm adalah bilangan riel positif dan xixi dan $ y_i$ adalah dua vektor dalam runang dimensi nn Implementasi ukuran jarak Minkowski pada model clustering data atribut dilakukan normalisasi untuk menghindari dominasi dari atribut yang memiliki skala data besar. Langkah-Langkah Mengukur jarak: sebelumnya kita ambil data dari website https://archive.ics.uci.edu/ml/datasets/wholesale+customers Download datanya dan kita jalankan python , tulis script seperti di bawah ini digunakan untuk mengambil data 4 baris : import pandas as pd from scipy import stats df = pd . read_csv ( 'Wholesale customers data.csv' , nrows = 4 , sep = ';' ) df Hasil dari script di atas: Channel Region Fresh Milk Grocery Frozen Detergents_Paper Delicassen 0 2 3 12669 9656 7561 214 2674 1338 1 2 3 7057 9810 9568 1762 3293 1776 2 2 3 6353 8808 7684 2405 3516 7844 3 1 3 13265 1196 4221 6404 507 1788 Menghitung Jarak antar data: ```python import scipy.spatial.distance as minko import itertools def minkowski (x,y,data): return sum(x)+sum(y) dfvalues = df.values.tolist() data = [ [x[0],x[1],minko.minkowski(dfvalues[x[0]], dfvalues[x[1]], 1) ,minko.minkowski(dfvalues[x[0]], dfvalues[x[1]], 2)] for x in itertools.combinations(range(4),2) ] columns = ['x','y', 'Minkowski (m-1)', 'Minkowski (m-2)'] pd.DataFrame(data, coluns=columns) ``` Hasil dari script di atas: x y Minkowski (m-1) Minkowski (m-2) 0 0 1 10378.0 6206.256360 1 0 2 16826.0 9405.507429 2 0 3 21204.0 11238.189623 3 1 2 10524.0 6506.372107 4 1 3 27610.0 13062.954260 5 2 3 31052.0 13395.218401 Langkah di bawah ini Mengukur Jarak Antara Numerick python numerical=[0,3] categorical=[1,2,6,7] binary=[4,5,8] ordinal=[1,2] on def chordDist(v1,v2,jnis): jmlh=0 normv1=0 normv2=0 for x in range (len(jnis)): normv1=normv1+(int(df.values.tolist()[v1][jnis[x]])**2) normv2=normv2+(int(df.values.tolist()[v1][jnis[x]])**2) jmlh=jmlh+(int(df.values.tolist()[v1][jnis[x]])*int(df.values.tolist()[v2][jnis[x]])) return ((2-(2*jmlh/(normv1*normv2)))**0.5) Hasil dari script di atas: x y Jarak Numeric Ordinal Categorical Binary 0 2 0 1.41 0 0 0 0 3 0 1.41 0 0 0 1 2 0 1.41 0 0 0 1 3 0 1.41 0 0 0 2 3 0 1.41 0 0 0 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Tugas 2 - mengukur jarak"},{"location":"Mengukur Jarak/#pengertian","text":"Salah satu tantangan dalam era ini dengan datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Alogritma seperit Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkan.","title":"Pengertian"},{"location":"Mengukur Jarak/#mengukur-jarak-menggunakan-minkowski-distance","text":"Langkah-langkah mengukur jarak kali ini saya menggunakan Minkowski Distance . Minkowski Distance adalah matrik dalam ruang vektor normed yang dapat dianggap sebagai generalisasi dari Euclidean Distance dan Manhattan Distance. $$ d _ { \\operatorname { min } } = ( \\ sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 $$ diman mm adalah bilangan riel positif dan xixi dan $ y_i$ adalah dua vektor dalam runang dimensi nn Implementasi ukuran jarak Minkowski pada model clustering data atribut dilakukan normalisasi untuk menghindari dominasi dari atribut yang memiliki skala data besar. Langkah-Langkah Mengukur jarak: sebelumnya kita ambil data dari website https://archive.ics.uci.edu/ml/datasets/wholesale+customers Download datanya dan kita jalankan python , tulis script seperti di bawah ini digunakan untuk mengambil data 4 baris : import pandas as pd from scipy import stats df = pd . read_csv ( 'Wholesale customers data.csv' , nrows = 4 , sep = ';' ) df Hasil dari script di atas: Channel Region Fresh Milk Grocery Frozen Detergents_Paper Delicassen 0 2 3 12669 9656 7561 214 2674 1338 1 2 3 7057 9810 9568 1762 3293 1776 2 2 3 6353 8808 7684 2405 3516 7844 3 1 3 13265 1196 4221 6404 507 1788 Menghitung Jarak antar data: ```python import scipy.spatial.distance as minko import itertools def minkowski (x,y,data): return sum(x)+sum(y) dfvalues = df.values.tolist() data = [ [x[0],x[1],minko.minkowski(dfvalues[x[0]], dfvalues[x[1]], 1) ,minko.minkowski(dfvalues[x[0]], dfvalues[x[1]], 2)] for x in itertools.combinations(range(4),2) ] columns = ['x','y', 'Minkowski (m-1)', 'Minkowski (m-2)'] pd.DataFrame(data, coluns=columns) ``` Hasil dari script di atas: x y Minkowski (m-1) Minkowski (m-2) 0 0 1 10378.0 6206.256360 1 0 2 16826.0 9405.507429 2 0 3 21204.0 11238.189623 3 1 2 10524.0 6506.372107 4 1 3 27610.0 13062.954260 5 2 3 31052.0 13395.218401 Langkah di bawah ini Mengukur Jarak Antara Numerick python numerical=[0,3] categorical=[1,2,6,7] binary=[4,5,8] ordinal=[1,2] on def chordDist(v1,v2,jnis): jmlh=0 normv1=0 normv2=0 for x in range (len(jnis)): normv1=normv1+(int(df.values.tolist()[v1][jnis[x]])**2) normv2=normv2+(int(df.values.tolist()[v1][jnis[x]])**2) jmlh=jmlh+(int(df.values.tolist()[v1][jnis[x]])*int(df.values.tolist()[v2][jnis[x]])) return ((2-(2*jmlh/(normv1*normv2)))**0.5) Hasil dari script di atas: x y Jarak Numeric Ordinal Categorical Binary 0 2 0 1.41 0 0 0 0 3 0 1.41 0 0 0 1 2 0 1.41 0 0 0 1 3 0 1.41 0 0 0 2 3 0 1.41 0 0 0 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Mengukur Jarak Menggunakan Minkowski Distance"},{"location":"Selection Fiture/","text":"Seleksi Fitur \u00b6 Seleksi fitur adalah teknik untuk memilih fitur penting dan relevan terhadap data dan mengurangi fitur yang tidak relevan. Seleksi fitur bertujuan untuk memilih fitur terbaik dari suatu kumpulan data fitur. Seleksi fitur bertujuan untuk memilih fitur terbaik dari suatu kumpulan data fitur. Tujuan dari penelitian ini adalah menerapkan metode Information Gain dalam sistem seleksi fitur untuk Permasalahan cuaca . Metode Information Gain adalah metode yang menggunakan teknik scoring untuk pembobotan sebuah fitur dengan menggunakan maksimal entropy. Fitur yang dipilih adalah fitur dengan nilai Information Gain yang lebih besar atau sama dengan nilai threshold tertentu. contoh data permasalahan cuaca from pandas import * from IPython.display import HTML , display from tabulate import tabulate from math import log from sklearn.feature_selection import mutual_info_classif def table ( df ): display ( HTML ( tabulate ( df , tablefmt = 'html' , headers = 'keys' , showindex = False ))) df = read_csv ( 'featureselection.csv' , usecols = [ 0 , 1 , 2 , 3 , 4 ], sep = ';' ) table ( df ) outlook temperature humidity windy play sunny hot high False no sunny hot high True no overcast hot high False yes rainy mild high False yes rainy cool normal False yes rainy cool normal True no overcast cool normal True yes sunny mild high False no sunny cool normal False yes rainy mild normal False yes sunny mild normal True yes overcast mild high True yes overcast hot normal False yes rainy mild high True no Mencari Entropy \u00b6 Untuk menghitung Information gain perlu dihitung dahulu nilai informasi dalam suatu bits dari suatu kumpulan obyek. Cara penghitungan dilakukan dengan menggunakan konsep entropi. Entropi menyatakan impurity suatu kumpulan obyek . Berikut merupakan definisi dari entropi suatu ruang sampel data (S): $$ E(T) = \\sum_{i=1}^n {-P_i\\log{P_i}} $$ dimana : T = ruang sampel data yang di gunakaan untuk data pelatihan Pi = Probability muncul dalam row def findEntropy ( column ): rawGroups = df . groupby ( column ) targetGroups = [[ key , len ( data ), len ( data ) / df [ column ] . size ] for key , data in rawGroups ] targetGroups = DataFrame ( targetGroups , columns = [ 'value' , 'count' , 'probability' ]) return sum ([ - x * log ( x , 2 ) for x in targetGroups [ 'probability' ]]), targetGroups , rawGroups entropyTarget , groupTargets , _ = findEntropy ( 'play' ) table ( groupTargets ) print ( 'entropy target =' , entropyTarget ) Entropi \u00b6 value count probability no 5 0.357143 yes 9 0.642857 entropy target = 0.9402859586706309 Gain \u00b6 Gain adalah sebuah fiktur yang terdapat pada sebuah data , untuk menghitungnya contoh rumusnya : $$ \\operatorname{Gain}(T, X) = \\operatorname{Entropy}(T) - \\sum_{v\\in{T}} \\frac{T_{X,v}}{T} E(T_{X,v}) $$ dimana : Entropy (T) = nilai entropi total dari atribut keputusan dalam ruang sampel data T x = fitur def findGain ( column ): entropyOutlook , groupOutlooks , rawOutlooks = findEntropy ( column ) table ( groupOutlooks ) gain = entropyTarget - sum ( len ( data ) / len ( df ) * sum ( - x / len ( data ) * log ( x / len ( data ), 2 ) for x in data . groupby ( 'play' ) . size ()) for key , data in rawOutlooks ) print ( \"gain of\" , column , \"is\" , gain ) return gain gains = [[ x , findGain ( x )] for x in [ 'outlook' , 'temperature' , 'humidity' , 'windy' ]] Outlook \u00b6 value count probability overcast 4 0.285714 rainy 5 0.357143 sunny 5 0.357143 gain of outlook is 0.2467498197744391 Temperature \u00b6 value count probability cool 4 0.285714 hot 4 0.285714 mild 6 0.428571 gain of temperature is 0.029222565658954647 Humidity \u00b6 value count probability high 7 0.5 normal 7 0.5 gain of humidity is 0.15183550136234136 Windy \u00b6 value count probability False 8 0.571429 True 6 0.428571 gain of windy is 0.04812703040826927 Skor keseluruhan Gain \u00b6 Skor ini sudah di urutkan menurut tingkat tertingi dari sebuat semua Gain, tergantung dari kalian mau mengambil berapa banyak data yang terpenting , bisa 2 atau lebih. table ( DataFrame ( gains , columns = [ \"Feature\" , \"Gain Score\" ]) . sort_values ( \"Gain Score\" )[:: - 1 ]) Feature Gain Score outlook 0.24675 humidity 0.151836 windy 0.048127 temperature 0.0292226 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Tugas 3 - Selection Fiture"},{"location":"Selection Fiture/#seleksi-fitur","text":"Seleksi fitur adalah teknik untuk memilih fitur penting dan relevan terhadap data dan mengurangi fitur yang tidak relevan. Seleksi fitur bertujuan untuk memilih fitur terbaik dari suatu kumpulan data fitur. Seleksi fitur bertujuan untuk memilih fitur terbaik dari suatu kumpulan data fitur. Tujuan dari penelitian ini adalah menerapkan metode Information Gain dalam sistem seleksi fitur untuk Permasalahan cuaca . Metode Information Gain adalah metode yang menggunakan teknik scoring untuk pembobotan sebuah fitur dengan menggunakan maksimal entropy. Fitur yang dipilih adalah fitur dengan nilai Information Gain yang lebih besar atau sama dengan nilai threshold tertentu. contoh data permasalahan cuaca from pandas import * from IPython.display import HTML , display from tabulate import tabulate from math import log from sklearn.feature_selection import mutual_info_classif def table ( df ): display ( HTML ( tabulate ( df , tablefmt = 'html' , headers = 'keys' , showindex = False ))) df = read_csv ( 'featureselection.csv' , usecols = [ 0 , 1 , 2 , 3 , 4 ], sep = ';' ) table ( df ) outlook temperature humidity windy play sunny hot high False no sunny hot high True no overcast hot high False yes rainy mild high False yes rainy cool normal False yes rainy cool normal True no overcast cool normal True yes sunny mild high False no sunny cool normal False yes rainy mild normal False yes sunny mild normal True yes overcast mild high True yes overcast hot normal False yes rainy mild high True no","title":"Seleksi Fitur"},{"location":"Selection Fiture/#mencari-entropy","text":"Untuk menghitung Information gain perlu dihitung dahulu nilai informasi dalam suatu bits dari suatu kumpulan obyek. Cara penghitungan dilakukan dengan menggunakan konsep entropi. Entropi menyatakan impurity suatu kumpulan obyek . Berikut merupakan definisi dari entropi suatu ruang sampel data (S): $$ E(T) = \\sum_{i=1}^n {-P_i\\log{P_i}} $$ dimana : T = ruang sampel data yang di gunakaan untuk data pelatihan Pi = Probability muncul dalam row def findEntropy ( column ): rawGroups = df . groupby ( column ) targetGroups = [[ key , len ( data ), len ( data ) / df [ column ] . size ] for key , data in rawGroups ] targetGroups = DataFrame ( targetGroups , columns = [ 'value' , 'count' , 'probability' ]) return sum ([ - x * log ( x , 2 ) for x in targetGroups [ 'probability' ]]), targetGroups , rawGroups entropyTarget , groupTargets , _ = findEntropy ( 'play' ) table ( groupTargets ) print ( 'entropy target =' , entropyTarget )","title":"Mencari Entropy"},{"location":"Selection Fiture/#entropi","text":"value count probability no 5 0.357143 yes 9 0.642857 entropy target = 0.9402859586706309","title":"Entropi"},{"location":"Selection Fiture/#gain","text":"Gain adalah sebuah fiktur yang terdapat pada sebuah data , untuk menghitungnya contoh rumusnya : $$ \\operatorname{Gain}(T, X) = \\operatorname{Entropy}(T) - \\sum_{v\\in{T}} \\frac{T_{X,v}}{T} E(T_{X,v}) $$ dimana : Entropy (T) = nilai entropi total dari atribut keputusan dalam ruang sampel data T x = fitur def findGain ( column ): entropyOutlook , groupOutlooks , rawOutlooks = findEntropy ( column ) table ( groupOutlooks ) gain = entropyTarget - sum ( len ( data ) / len ( df ) * sum ( - x / len ( data ) * log ( x / len ( data ), 2 ) for x in data . groupby ( 'play' ) . size ()) for key , data in rawOutlooks ) print ( \"gain of\" , column , \"is\" , gain ) return gain gains = [[ x , findGain ( x )] for x in [ 'outlook' , 'temperature' , 'humidity' , 'windy' ]]","title":"Gain"},{"location":"Selection Fiture/#outlook","text":"value count probability overcast 4 0.285714 rainy 5 0.357143 sunny 5 0.357143 gain of outlook is 0.2467498197744391","title":"Outlook"},{"location":"Selection Fiture/#temperature","text":"value count probability cool 4 0.285714 hot 4 0.285714 mild 6 0.428571 gain of temperature is 0.029222565658954647","title":"Temperature"},{"location":"Selection Fiture/#humidity","text":"value count probability high 7 0.5 normal 7 0.5 gain of humidity is 0.15183550136234136","title":"Humidity"},{"location":"Selection Fiture/#windy","text":"value count probability False 8 0.571429 True 6 0.428571 gain of windy is 0.04812703040826927","title":"Windy"},{"location":"Selection Fiture/#skor-keseluruhan-gain","text":"Skor ini sudah di urutkan menurut tingkat tertingi dari sebuat semua Gain, tergantung dari kalian mau mengambil berapa banyak data yang terpenting , bisa 2 atau lebih. table ( DataFrame ( gains , columns = [ \"Feature\" , \"Gain Score\" ]) . sort_values ( \"Gain Score\" )[:: - 1 ]) Feature Gain Score outlook 0.24675 humidity 0.151836 windy 0.048127 temperature 0.0292226 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Skor keseluruhan Gain"},{"location":"knn/","text":"W-KNN (Weighted K-Nearest Neighbour) \u00b6 Mengambil Sample data: sepal_length sepal_width petal_length petal_width species 4 3,5 4,2 1,7 versicolor Data iris : sepal_length as A sepal_width as B petal_length as C petal_width as D species 6,9 3,1 5,1 2,3 virginica 6,7 3 5 1,7 versicolor 6,9 3,1 4,9 1,5 versicolor 6,7 3 5,2 2,3 virginica 6,7 3,1 4,7 1,5 versicolor 6,5 3,2 5,1 2 virginica 6,9 3,1 5,4 2,1 virginica 7 3,2 4,7 1,4 versicolor 6,5 3 5,2 2 virginica 6,8 2,8 4,8 1,4 versicolor 6,8 3 5,5 2,1 virginica 6,5 2,8 4,6 1,5 versicolor 6,7 3,1 4,4 1,4 versicolor 6,3 2,7 4,9 1,8 virginica 6,6 3 4,4 1,4 versicolor 6,6 2,9 4,6 1,3 versicolor 6,4 2,7 5,3 1,9 virginica 6,2 2,8 4,8 1,8 virginica 6,4 3,2 4,5 1,5 versicolor 6,4 3,2 5,3 2,3 virginica 6,3 3,3 4,7 1,6 versicolor 6,5 3 5,5 1,8 virginica 6,3 2,5 5 1,9 virginica 6,1 3 4,9 1,8 virginica 6,3 2,8 5,1 1,5 virginica 6,7 3,1 5,6 2,4 virginica 6,4 3,1 5,5 1,8 virginica 6,9 3,2 5,7 2,3 virginica 6,3 2,5 4,9 1,5 versicolor 6,7 3,3 5,7 2,1 virginica 6 3 4,8 1,8 virginica 6,4 2,8 5,6 2,1 virginica 6,4 2,8 5,6 2,2 virginica 6,4 2,9 4,3 1,3 versicolor 6,3 2,9 5,6 1,8 virginica 6,1 2,9 4,7 1,4 versicolor 7,2 3 5,8 1,6 virginica 6,1 3 4,6 1,4 versicolor 7,1 3 5,9 2,1 virginica 6,7 3,3 5,7 2,5 virginica 6,2 3,4 5,4 2,3 virginica 5,9 3,2 4,8 1,8 versicolor 6,5 3 5,8 2,2 virginica 5,9 3 5,1 1,8 virginica 6 2,7 5,1 1,6 versicolor 6 2,9 4,5 1,5 versicolor 6,7 2,5 5,8 1,8 virginica 6,8 3,2 5,9 2,3 virginica 6,2 2,9 4,3 1,3 versicolor 6 3,4 4,5 1,6 versicolor 6,3 3,4 5,6 2,4 virginica 6,1 2,8 4,7 1,2 versicolor 7,2 3,2 6 1,8 virginica 6,2 2,2 4,5 1,5 versicolor 6,3 2,3 4,4 1,3 versicolor 5,8 2,7 5,1 1,9 virginica 5,8 2,7 5,1 1,9 virginica 5,8 2,8 5,1 2,4 virginica 5,9 3 4,2 1,5 versicolor 7,4 2,8 6,1 1,9 virginica 6 2,2 5 1,5 virginica 6,1 2,6 5,6 1,4 virginica 5,7 2,5 5 2 virginica 6,1 2,8 4 1,3 versicolor 5,6 2,8 4,9 2 virginica 7,7 3 6,1 2,3 virginica 6,3 3,3 6 2,5 virginica 5,6 3 4,5 1,5 versicolor 5,7 2,8 4,5 1,3 versicolor 7,2 3,6 6,1 2,5 virginica 7,3 2,9 6,3 1,8 virginica 5,7 2,9 4,2 1,3 versicolor 5,7 3 4,2 1,2 versicolor 5,7 2,8 4,1 1,3 versicolor 5,8 2,6 4 1,2 versicolor 5,4 3 4,5 1,5 versicolor 5,6 2,7 4,2 1,3 versicolor 5,6 3 4,1 1,3 versicolor 5,8 2,7 3,9 1,2 versicolor 5,8 2,7 4,1 1 versicolor 5,5 2,6 4,4 1,2 versicolor 6 2,2 4 1 versicolor 7,6 3 6,6 2,1 virginica 5,5 2,5 4 1,3 versicolor 5,6 2,5 3,9 1,1 versicolor 5,5 2,3 4 1,3 versicolor 5,6 2,9 3,6 1,3 versicolor 7,9 3,8 6,4 2 virginica 7,7 2,8 6,7 2 virginica 5,5 2,4 3,8 1,1 versicolor 5,2 2,7 3,9 1,4 versicolor 5,7 2,6 3,5 1 versicolor 7,7 3,8 6,7 2,2 virginica 5,5 2,4 3,7 1 versicolor 4,9 2,5 4,5 1,7 virginica 7,7 2,6 6,9 2,3 virginica 5 2 3,5 1 versicolor 5 2,3 3,3 1 versicolor 5,1 2,5 3 1,1 versicolor 4,9 2,4 3,3 1 versicolor 5,7 3,8 1,7 0,3 setosa 5,1 3,8 1,9 0,4 setosa 5,4 3,9 1,7 0,4 setosa 5,1 3,3 1,7 0,5 setosa 5,4 3,4 1,7 0,2 setosa 5,4 3,4 1,5 0,4 setosa 5 3,5 1,6 0,6 setosa 4,8 3,4 1,9 0,2 setosa 5 3,4 1,6 0,4 setosa 5,7 4,4 1,5 0,4 setosa 5,4 3,7 1,5 0,2 setosa 5 3 1,6 0,2 setosa 5,3 3,7 1,5 0,2 setosa 5,1 3,7 1,5 0,4 setosa 5,2 3,5 1,5 0,2 setosa 5,1 3,8 1,6 0,2 setosa 5,1 3,4 1,5 0,2 setosa 5,5 3,5 1,3 0,2 setosa 5,1 3,8 1,5 0,3 setosa 5,4 3,9 1,3 0,4 setosa 5,2 3,4 1,4 0,2 setosa 5 3,4 1,5 0,2 setosa 5,1 3,5 1,4 0,3 setosa 4,8 3,1 1,6 0,2 setosa 4,8 3,4 1,6 0,2 setosa 5,5 4,2 1,4 0,2 setosa 5,8 4 1,2 0,2 setosa 5,1 3,5 1,4 0,2 setosa 4,7 3,2 1,6 0,2 setosa 5 3,3 1,4 0,2 setosa 4,9 3,1 1,5 0,1 setosa 4,9 3,1 1,5 0,1 setosa 4,9 3,1 1,5 0,1 setosa 5,2 4,1 1,5 0,1 setosa 5 3,6 1,4 0,2 setosa 4,9 3 1,4 0,2 setosa 4,8 3 1,4 0,3 setosa 5 3,5 1,3 0,3 setosa 4,6 3,1 1,5 0,2 setosa 4,8 3 1,4 0,1 setosa 5 3,2 1,2 0,2 setosa 4,6 3,4 1,4 0,3 setosa 4,6 3,2 1,4 0,2 setosa 4,7 3,2 1,3 0,2 setosa 4,4 2,9 1,4 0,2 setosa 4,5 2,3 1,3 0,3 setosa 4,4 3 1,3 0,2 setosa 4,4 3,2 1,3 0,2 setosa 4,6 3,6 1 0,2 setosa 4,3 3 1,1 0,1 setosa Menghitung jarak dengan sample data : (A-S1)^2 (B-S2)^2 (C-S3)^2 (D-S4)^2 SQRT(D) 8,4 0,2 0,8 0,4 3,1 7,3 0,3 0,6 0,0 2,9 8,4 0,2 0,5 0,0 3,0 7,3 0,3 1,0 0,4 3,0 7,3 0,2 0,3 0,0 2,8 6,3 0,1 0,8 0,1 2,7 8,4 0,2 1,4 0,2 3,2 9,0 0,1 0,3 0,1 3,1 6,3 0,3 1,0 0,1 2,8 7,8 0,5 0,4 0,1 3,0 7,8 0,3 1,7 0,2 3,2 6,3 0,5 0,2 0,0 2,6 7,3 0,2 0,0 0,1 2,8 5,3 0,6 0,5 0,0 2,5 6,8 0,3 0,0 0,1 2,7 6,8 0,4 0,2 0,2 2,7 5,8 0,6 1,2 0,0 2,8 4,8 0,5 0,4 0,0 2,4 5,8 0,1 0,1 0,0 2,4 5,8 0,1 1,2 0,4 2,7 5,3 0,0 0,3 0,0 2,4 6,3 0,3 1,7 0,0 2,9 5,3 1,0 0,6 0,0 2,6 4,4 0,3 0,5 0,0 2,3 5,3 0,5 0,8 0,0 2,6 7,3 0,2 2,0 0,5 3,1 5,8 0,2 1,7 0,0 2,8 8,4 0,1 2,3 0,4 3,3 5,3 1,0 0,5 0,0 2,6 7,3 0,0 2,3 0,2 3,1 4,0 0,3 0,4 0,0 2,1 5,8 0,5 2,0 0,2 2,9 5,8 0,5 2,0 0,3 2,9 5,8 0,4 0,0 0,2 2,5 5,3 0,4 2,0 0,0 2,8 4,4 0,4 0,3 0,1 2,3 10,2 0,3 2,6 0,0 3,6 4,4 0,3 0,2 0,1 2,2 9,6 0,3 2,9 0,2 3,6 7,3 0,0 2,3 0,6 3,2 4,8 0,0 1,4 0,4 2,6 3,6 0,1 0,4 0,0 2,0 6,3 0,3 2,6 0,3 3,1 3,6 0,3 0,8 0,0 2,2 4,0 0,6 0,8 0,0 2,3 4,0 0,4 0,1 0,0 2,1 7,3 1,0 2,6 0,0 3,3 7,8 0,1 2,9 0,4 3,3 4,8 0,4 0,0 0,2 2,3 4,0 0,0 0,1 0,0 2,0 5,3 0,0 2,0 0,5 2,8 4,4 0,5 0,3 0,3 2,3 10,2 0,1 3,2 0,0 3,7 4,8 1,7 0,1 0,0 2,6 5,3 1,4 0,0 0,2 2,6 3,2 0,6 0,8 0,0 2,2 3,2 0,6 0,8 0,0 2,2 3,2 0,5 0,8 0,5 2,2 3,6 0,3 0,0 0,0 2,0 11,6 0,5 3,6 0,0 4,0 4,0 1,7 0,6 0,0 2,5 4,4 0,8 2,0 0,1 2,7 2,9 1,0 0,6 0,1 2,1 4,4 0,5 0,0 0,2 2,3 2,6 0,5 0,5 0,1 1,9 13,7 0,3 3,6 0,4 4,2 5,3 0,0 3,2 0,6 3,0 2,6 0,3 0,1 0,0 1,7 2,9 0,5 0,1 0,2 1,9 10,2 0,0 3,6 0,6 3,8 10,9 0,4 4,4 0,0 4,0 2,9 0,4 0,0 0,2 1,8 2,9 0,3 0,0 0,3 1,8 2,9 0,5 0,0 0,2 1,9 3,2 0,8 0,0 0,3 2,1 2,0 0,3 0,1 0,0 1,5 2,6 0,6 0,0 0,2 1,8 2,6 0,3 0,0 0,2 1,7 3,2 0,6 0,1 0,3 2,1 3,2 0,6 0,0 0,5 2,1 2,3 0,8 0,0 0,3 1,8 4,0 1,7 0,0 0,5 2,5 13,0 0,3 5,8 0,2 4,4 2,3 1,0 0,0 0,2 1,9 2,6 1,0 0,1 0,4 2,0 2,3 1,4 0,0 0,2 2,0 2,6 0,4 0,4 0,2 1,9 15,2 0,1 4,8 0,1 4,5 13,7 0,5 6,3 0,1 4,5 2,3 1,2 0,2 0,4 2,0 1,4 0,6 0,1 0,1 1,5 2,9 0,8 0,5 0,5 2,2 13,7 0,1 6,3 0,3 4,5 2,3 1,2 0,3 0,5 2,0 0,8 1,0 0,1 0,0 1,4 13,7 0,8 7,3 0,4 4,7 1,0 2,3 0,5 0,5 2,1 1,0 1,4 0,8 0,5 1,9 1,2 1,0 1,4 0,4 2,0 0,8 1,2 0,8 0,5 1,8 2,9 0,1 6,3 2,0 3,3 1,2 0,1 5,3 1,7 2,9 2,0 0,2 6,3 1,7 3,2 1,2 0,0 6,3 1,4 3,0 2,0 0,0 6,3 2,3 3,2 2,0 0,0 7,3 1,7 3,3 1,0 0,0 6,8 1,2 3,0 0,6 0,0 5,3 2,3 2,9 1,0 0,0 6,8 1,7 3,1 2,9 0,8 7,3 1,7 3,6 2,0 0,0 7,3 2,3 3,4 1,0 0,3 6,8 2,3 3,2 1,7 0,0 7,3 2,3 3,4 1,2 0,0 7,3 1,7 3,2 1,4 0,0 7,3 2,3 3,3 1,2 0,1 6,8 2,3 3,2 1,2 0,0 7,3 2,3 3,3 2,3 0,0 8,4 2,3 3,6 1,2 0,1 7,3 2,0 3,2 2,0 0,2 8,4 1,7 3,5 1,4 0,0 7,8 2,3 3,4 1,0 0,0 7,3 2,3 3,2 1,2 0,0 7,8 2,0 3,3 0,6 0,2 6,8 2,3 3,1 0,6 0,0 6,8 2,3 3,1 2,3 0,5 7,8 2,3 3,6 3,2 0,3 9,0 2,3 3,8 1,2 0,0 7,8 2,3 3,4 0,5 0,1 6,8 2,3 3,1 1,0 0,0 7,8 2,3 3,3 0,8 0,2 7,3 2,6 3,3 0,8 0,2 7,3 2,6 3,3 0,8 0,2 7,3 2,6 3,3 1,4 0,4 7,3 2,6 3,4 1,0 0,0 7,8 2,3 3,3 0,8 0,3 7,8 2,3 3,3 0,6 0,3 7,8 2,0 3,3 1,0 0,0 8,4 2,0 3,4 0,4 0,2 7,3 2,3 3,2 0,6 0,3 7,8 2,6 3,4 1,0 0,1 9,0 2,3 3,5 0,4 0,0 7,8 2,0 3,2 0,4 0,1 7,8 2,3 3,2 0,5 0,1 8,4 2,3 3,4 0,2 0,4 7,8 2,3 3,3 0,3 1,4 8,4 2,0 3,5 0,2 0,3 8,4 2,3 3,3 0,2 0,1 8,4 2,3 3,3 0,4 0,0 10,2 2,3 3,6 0,1 0,3 9,6 2,6 3,5 Mencari data tertinggi K = 5: sepal_length as A sepal_width as B petal_length as C petal_width as D species (A-S1)^2 (B-S2)^2 (C-S3)^2 (D-S4)^2 SQRT(D) 5,6 3 4,5 1,5 versicolor 2,6 0,3 0,1 0,0 1,7 5,4 3 4,5 1,5 versicolor 2,0 0,3 0,1 0,0 1,5 5,6 3 4,1 1,3 versicolor 2,6 0,3 0,0 0,2 1,7 5,2 2,7 3,9 1,4 versicolor 1,4 0,6 0,1 0,1 1,5 4,9 2,5 4,5 1,7 virginica 0,8 1,0 0,1 0,0 1,4 Menghitung berat antar variasi dengan 1/jaraknya : D 1/D Setosa versicolor virginica 1,4 0,725476 0 0 0,725476 1,5 0,65372 0 0,65372 0 1,5 0,66519 0 0,66519 0 1,7 0,583212 0 0,583212 0 1,7 0,579284 0 0,579284 0 sum 0 2,481407 0,725476 Jadi Nilai terbesar adalah : class = versicolor","title":"Tugas 5 - W-KNN (Weighted K-Nearest Neighbour)"},{"location":"knn/#w-knn-weighted-k-nearest-neighbour","text":"Mengambil Sample data: sepal_length sepal_width petal_length petal_width species 4 3,5 4,2 1,7 versicolor Data iris : sepal_length as A sepal_width as B petal_length as C petal_width as D species 6,9 3,1 5,1 2,3 virginica 6,7 3 5 1,7 versicolor 6,9 3,1 4,9 1,5 versicolor 6,7 3 5,2 2,3 virginica 6,7 3,1 4,7 1,5 versicolor 6,5 3,2 5,1 2 virginica 6,9 3,1 5,4 2,1 virginica 7 3,2 4,7 1,4 versicolor 6,5 3 5,2 2 virginica 6,8 2,8 4,8 1,4 versicolor 6,8 3 5,5 2,1 virginica 6,5 2,8 4,6 1,5 versicolor 6,7 3,1 4,4 1,4 versicolor 6,3 2,7 4,9 1,8 virginica 6,6 3 4,4 1,4 versicolor 6,6 2,9 4,6 1,3 versicolor 6,4 2,7 5,3 1,9 virginica 6,2 2,8 4,8 1,8 virginica 6,4 3,2 4,5 1,5 versicolor 6,4 3,2 5,3 2,3 virginica 6,3 3,3 4,7 1,6 versicolor 6,5 3 5,5 1,8 virginica 6,3 2,5 5 1,9 virginica 6,1 3 4,9 1,8 virginica 6,3 2,8 5,1 1,5 virginica 6,7 3,1 5,6 2,4 virginica 6,4 3,1 5,5 1,8 virginica 6,9 3,2 5,7 2,3 virginica 6,3 2,5 4,9 1,5 versicolor 6,7 3,3 5,7 2,1 virginica 6 3 4,8 1,8 virginica 6,4 2,8 5,6 2,1 virginica 6,4 2,8 5,6 2,2 virginica 6,4 2,9 4,3 1,3 versicolor 6,3 2,9 5,6 1,8 virginica 6,1 2,9 4,7 1,4 versicolor 7,2 3 5,8 1,6 virginica 6,1 3 4,6 1,4 versicolor 7,1 3 5,9 2,1 virginica 6,7 3,3 5,7 2,5 virginica 6,2 3,4 5,4 2,3 virginica 5,9 3,2 4,8 1,8 versicolor 6,5 3 5,8 2,2 virginica 5,9 3 5,1 1,8 virginica 6 2,7 5,1 1,6 versicolor 6 2,9 4,5 1,5 versicolor 6,7 2,5 5,8 1,8 virginica 6,8 3,2 5,9 2,3 virginica 6,2 2,9 4,3 1,3 versicolor 6 3,4 4,5 1,6 versicolor 6,3 3,4 5,6 2,4 virginica 6,1 2,8 4,7 1,2 versicolor 7,2 3,2 6 1,8 virginica 6,2 2,2 4,5 1,5 versicolor 6,3 2,3 4,4 1,3 versicolor 5,8 2,7 5,1 1,9 virginica 5,8 2,7 5,1 1,9 virginica 5,8 2,8 5,1 2,4 virginica 5,9 3 4,2 1,5 versicolor 7,4 2,8 6,1 1,9 virginica 6 2,2 5 1,5 virginica 6,1 2,6 5,6 1,4 virginica 5,7 2,5 5 2 virginica 6,1 2,8 4 1,3 versicolor 5,6 2,8 4,9 2 virginica 7,7 3 6,1 2,3 virginica 6,3 3,3 6 2,5 virginica 5,6 3 4,5 1,5 versicolor 5,7 2,8 4,5 1,3 versicolor 7,2 3,6 6,1 2,5 virginica 7,3 2,9 6,3 1,8 virginica 5,7 2,9 4,2 1,3 versicolor 5,7 3 4,2 1,2 versicolor 5,7 2,8 4,1 1,3 versicolor 5,8 2,6 4 1,2 versicolor 5,4 3 4,5 1,5 versicolor 5,6 2,7 4,2 1,3 versicolor 5,6 3 4,1 1,3 versicolor 5,8 2,7 3,9 1,2 versicolor 5,8 2,7 4,1 1 versicolor 5,5 2,6 4,4 1,2 versicolor 6 2,2 4 1 versicolor 7,6 3 6,6 2,1 virginica 5,5 2,5 4 1,3 versicolor 5,6 2,5 3,9 1,1 versicolor 5,5 2,3 4 1,3 versicolor 5,6 2,9 3,6 1,3 versicolor 7,9 3,8 6,4 2 virginica 7,7 2,8 6,7 2 virginica 5,5 2,4 3,8 1,1 versicolor 5,2 2,7 3,9 1,4 versicolor 5,7 2,6 3,5 1 versicolor 7,7 3,8 6,7 2,2 virginica 5,5 2,4 3,7 1 versicolor 4,9 2,5 4,5 1,7 virginica 7,7 2,6 6,9 2,3 virginica 5 2 3,5 1 versicolor 5 2,3 3,3 1 versicolor 5,1 2,5 3 1,1 versicolor 4,9 2,4 3,3 1 versicolor 5,7 3,8 1,7 0,3 setosa 5,1 3,8 1,9 0,4 setosa 5,4 3,9 1,7 0,4 setosa 5,1 3,3 1,7 0,5 setosa 5,4 3,4 1,7 0,2 setosa 5,4 3,4 1,5 0,4 setosa 5 3,5 1,6 0,6 setosa 4,8 3,4 1,9 0,2 setosa 5 3,4 1,6 0,4 setosa 5,7 4,4 1,5 0,4 setosa 5,4 3,7 1,5 0,2 setosa 5 3 1,6 0,2 setosa 5,3 3,7 1,5 0,2 setosa 5,1 3,7 1,5 0,4 setosa 5,2 3,5 1,5 0,2 setosa 5,1 3,8 1,6 0,2 setosa 5,1 3,4 1,5 0,2 setosa 5,5 3,5 1,3 0,2 setosa 5,1 3,8 1,5 0,3 setosa 5,4 3,9 1,3 0,4 setosa 5,2 3,4 1,4 0,2 setosa 5 3,4 1,5 0,2 setosa 5,1 3,5 1,4 0,3 setosa 4,8 3,1 1,6 0,2 setosa 4,8 3,4 1,6 0,2 setosa 5,5 4,2 1,4 0,2 setosa 5,8 4 1,2 0,2 setosa 5,1 3,5 1,4 0,2 setosa 4,7 3,2 1,6 0,2 setosa 5 3,3 1,4 0,2 setosa 4,9 3,1 1,5 0,1 setosa 4,9 3,1 1,5 0,1 setosa 4,9 3,1 1,5 0,1 setosa 5,2 4,1 1,5 0,1 setosa 5 3,6 1,4 0,2 setosa 4,9 3 1,4 0,2 setosa 4,8 3 1,4 0,3 setosa 5 3,5 1,3 0,3 setosa 4,6 3,1 1,5 0,2 setosa 4,8 3 1,4 0,1 setosa 5 3,2 1,2 0,2 setosa 4,6 3,4 1,4 0,3 setosa 4,6 3,2 1,4 0,2 setosa 4,7 3,2 1,3 0,2 setosa 4,4 2,9 1,4 0,2 setosa 4,5 2,3 1,3 0,3 setosa 4,4 3 1,3 0,2 setosa 4,4 3,2 1,3 0,2 setosa 4,6 3,6 1 0,2 setosa 4,3 3 1,1 0,1 setosa Menghitung jarak dengan sample data : (A-S1)^2 (B-S2)^2 (C-S3)^2 (D-S4)^2 SQRT(D) 8,4 0,2 0,8 0,4 3,1 7,3 0,3 0,6 0,0 2,9 8,4 0,2 0,5 0,0 3,0 7,3 0,3 1,0 0,4 3,0 7,3 0,2 0,3 0,0 2,8 6,3 0,1 0,8 0,1 2,7 8,4 0,2 1,4 0,2 3,2 9,0 0,1 0,3 0,1 3,1 6,3 0,3 1,0 0,1 2,8 7,8 0,5 0,4 0,1 3,0 7,8 0,3 1,7 0,2 3,2 6,3 0,5 0,2 0,0 2,6 7,3 0,2 0,0 0,1 2,8 5,3 0,6 0,5 0,0 2,5 6,8 0,3 0,0 0,1 2,7 6,8 0,4 0,2 0,2 2,7 5,8 0,6 1,2 0,0 2,8 4,8 0,5 0,4 0,0 2,4 5,8 0,1 0,1 0,0 2,4 5,8 0,1 1,2 0,4 2,7 5,3 0,0 0,3 0,0 2,4 6,3 0,3 1,7 0,0 2,9 5,3 1,0 0,6 0,0 2,6 4,4 0,3 0,5 0,0 2,3 5,3 0,5 0,8 0,0 2,6 7,3 0,2 2,0 0,5 3,1 5,8 0,2 1,7 0,0 2,8 8,4 0,1 2,3 0,4 3,3 5,3 1,0 0,5 0,0 2,6 7,3 0,0 2,3 0,2 3,1 4,0 0,3 0,4 0,0 2,1 5,8 0,5 2,0 0,2 2,9 5,8 0,5 2,0 0,3 2,9 5,8 0,4 0,0 0,2 2,5 5,3 0,4 2,0 0,0 2,8 4,4 0,4 0,3 0,1 2,3 10,2 0,3 2,6 0,0 3,6 4,4 0,3 0,2 0,1 2,2 9,6 0,3 2,9 0,2 3,6 7,3 0,0 2,3 0,6 3,2 4,8 0,0 1,4 0,4 2,6 3,6 0,1 0,4 0,0 2,0 6,3 0,3 2,6 0,3 3,1 3,6 0,3 0,8 0,0 2,2 4,0 0,6 0,8 0,0 2,3 4,0 0,4 0,1 0,0 2,1 7,3 1,0 2,6 0,0 3,3 7,8 0,1 2,9 0,4 3,3 4,8 0,4 0,0 0,2 2,3 4,0 0,0 0,1 0,0 2,0 5,3 0,0 2,0 0,5 2,8 4,4 0,5 0,3 0,3 2,3 10,2 0,1 3,2 0,0 3,7 4,8 1,7 0,1 0,0 2,6 5,3 1,4 0,0 0,2 2,6 3,2 0,6 0,8 0,0 2,2 3,2 0,6 0,8 0,0 2,2 3,2 0,5 0,8 0,5 2,2 3,6 0,3 0,0 0,0 2,0 11,6 0,5 3,6 0,0 4,0 4,0 1,7 0,6 0,0 2,5 4,4 0,8 2,0 0,1 2,7 2,9 1,0 0,6 0,1 2,1 4,4 0,5 0,0 0,2 2,3 2,6 0,5 0,5 0,1 1,9 13,7 0,3 3,6 0,4 4,2 5,3 0,0 3,2 0,6 3,0 2,6 0,3 0,1 0,0 1,7 2,9 0,5 0,1 0,2 1,9 10,2 0,0 3,6 0,6 3,8 10,9 0,4 4,4 0,0 4,0 2,9 0,4 0,0 0,2 1,8 2,9 0,3 0,0 0,3 1,8 2,9 0,5 0,0 0,2 1,9 3,2 0,8 0,0 0,3 2,1 2,0 0,3 0,1 0,0 1,5 2,6 0,6 0,0 0,2 1,8 2,6 0,3 0,0 0,2 1,7 3,2 0,6 0,1 0,3 2,1 3,2 0,6 0,0 0,5 2,1 2,3 0,8 0,0 0,3 1,8 4,0 1,7 0,0 0,5 2,5 13,0 0,3 5,8 0,2 4,4 2,3 1,0 0,0 0,2 1,9 2,6 1,0 0,1 0,4 2,0 2,3 1,4 0,0 0,2 2,0 2,6 0,4 0,4 0,2 1,9 15,2 0,1 4,8 0,1 4,5 13,7 0,5 6,3 0,1 4,5 2,3 1,2 0,2 0,4 2,0 1,4 0,6 0,1 0,1 1,5 2,9 0,8 0,5 0,5 2,2 13,7 0,1 6,3 0,3 4,5 2,3 1,2 0,3 0,5 2,0 0,8 1,0 0,1 0,0 1,4 13,7 0,8 7,3 0,4 4,7 1,0 2,3 0,5 0,5 2,1 1,0 1,4 0,8 0,5 1,9 1,2 1,0 1,4 0,4 2,0 0,8 1,2 0,8 0,5 1,8 2,9 0,1 6,3 2,0 3,3 1,2 0,1 5,3 1,7 2,9 2,0 0,2 6,3 1,7 3,2 1,2 0,0 6,3 1,4 3,0 2,0 0,0 6,3 2,3 3,2 2,0 0,0 7,3 1,7 3,3 1,0 0,0 6,8 1,2 3,0 0,6 0,0 5,3 2,3 2,9 1,0 0,0 6,8 1,7 3,1 2,9 0,8 7,3 1,7 3,6 2,0 0,0 7,3 2,3 3,4 1,0 0,3 6,8 2,3 3,2 1,7 0,0 7,3 2,3 3,4 1,2 0,0 7,3 1,7 3,2 1,4 0,0 7,3 2,3 3,3 1,2 0,1 6,8 2,3 3,2 1,2 0,0 7,3 2,3 3,3 2,3 0,0 8,4 2,3 3,6 1,2 0,1 7,3 2,0 3,2 2,0 0,2 8,4 1,7 3,5 1,4 0,0 7,8 2,3 3,4 1,0 0,0 7,3 2,3 3,2 1,2 0,0 7,8 2,0 3,3 0,6 0,2 6,8 2,3 3,1 0,6 0,0 6,8 2,3 3,1 2,3 0,5 7,8 2,3 3,6 3,2 0,3 9,0 2,3 3,8 1,2 0,0 7,8 2,3 3,4 0,5 0,1 6,8 2,3 3,1 1,0 0,0 7,8 2,3 3,3 0,8 0,2 7,3 2,6 3,3 0,8 0,2 7,3 2,6 3,3 0,8 0,2 7,3 2,6 3,3 1,4 0,4 7,3 2,6 3,4 1,0 0,0 7,8 2,3 3,3 0,8 0,3 7,8 2,3 3,3 0,6 0,3 7,8 2,0 3,3 1,0 0,0 8,4 2,0 3,4 0,4 0,2 7,3 2,3 3,2 0,6 0,3 7,8 2,6 3,4 1,0 0,1 9,0 2,3 3,5 0,4 0,0 7,8 2,0 3,2 0,4 0,1 7,8 2,3 3,2 0,5 0,1 8,4 2,3 3,4 0,2 0,4 7,8 2,3 3,3 0,3 1,4 8,4 2,0 3,5 0,2 0,3 8,4 2,3 3,3 0,2 0,1 8,4 2,3 3,3 0,4 0,0 10,2 2,3 3,6 0,1 0,3 9,6 2,6 3,5 Mencari data tertinggi K = 5: sepal_length as A sepal_width as B petal_length as C petal_width as D species (A-S1)^2 (B-S2)^2 (C-S3)^2 (D-S4)^2 SQRT(D) 5,6 3 4,5 1,5 versicolor 2,6 0,3 0,1 0,0 1,7 5,4 3 4,5 1,5 versicolor 2,0 0,3 0,1 0,0 1,5 5,6 3 4,1 1,3 versicolor 2,6 0,3 0,0 0,2 1,7 5,2 2,7 3,9 1,4 versicolor 1,4 0,6 0,1 0,1 1,5 4,9 2,5 4,5 1,7 virginica 0,8 1,0 0,1 0,0 1,4 Menghitung berat antar variasi dengan 1/jaraknya : D 1/D Setosa versicolor virginica 1,4 0,725476 0 0 0,725476 1,5 0,65372 0 0,65372 0 1,5 0,66519 0 0,66519 0 1,7 0,583212 0 0,583212 0 1,7 0,579284 0 0,579284 0 sum 0 2,481407 0,725476 Jadi Nilai terbesar adalah : class = versicolor","title":"W-KNN (Weighted K-Nearest Neighbour)"},{"location":"naive bayes classifier/","text":"Naive Bayes Classifier \u00b6 Naive Bayes classifier (NBC) merupakan salah satu metoda pemelajaran mesin yang memanfaatkan perhitungan probabilitas dan statistik yang dikemukakan oleh ilmuwan Inggris Thomas Bayes , yaitu memprediksi probabilitas di masa depan berdasarkan pengalaman di masa sebelumnya. Keuntungan penggunan adalah bahwa metoda ini hanya membutuhkan jumlah data pelatihan ( training data ) yg kecil unt menentukan estimasi parameter yg diperlukan dalam proses pengklasifikasian. Karena yg diasumsikan sebagai variable independent, maka hanya varians dr suatu variable dalam sebuah kelas yg dibutuhkan unt menentukan klasifikasi, bukan keseluruhan dr matriks kovarians. Dasar teori naive bayes yang di pakai dalam pemrograman adalah rumus Bayes: $$ P(A|B) = \\frac{(P(B|A)P(A))}{P(B)} $$ Peluang Kejadian A sebagai B di tentukan dari peluang B saat A , peluang A, dan peluang B.Pada pengaplikasiannya nanti rumus ini berubah menjadi $$ P(\\operatorname{C_i|D}) = \\frac{\\left( P(D|C_i)\\right)P(C_i)}{P(D)} $$ Naive Bayes Classifier atau bisa di sebut sebagai multinomina naive bayes merupakan model penyederhanaan dari algoritma bayes yang cocok dalam pengklasifikasian text atau dokumen. P adalah probabilitas yang muncul. Untuk data numerik P adalah: $$ P(x=v|C_k) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_k}}\\exp\\left(-\\frac{(v-\\mu_k)^2}{2\\sigma^2_k}\\right) $$ dimana V adalah nilai dalam fitur. Tahapan Proses \u00b6 Ambil data Set \u00b6 ```python from sklearn import datasets from pandas import * from numpy import * from math import * from IPython.display import HTML, display; from tabulate import tabulate def table(df): display(HTML(tabulate(df, tablefmt='html', headers='keys', showindex=False))) ``` python iris = datasets.load_iris() data = [list(s)+[iris.target_names[iris.target[i]]] for i,s in enumerate(iris.data)] dataset = DataFrame(data, columns=iris.feature_names+['class']).sample(frac=0.2) table(dataset) sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 7.7 2.6 6.9 2.3 virginica 5.4 3.7 1.5 0.2 setosa 6.9 3.1 5.4 2.1 virginica 6.6 3 4.4 1.4 versicolor 4.6 3.1 1.5 0.2 setosa 5.1 3.4 1.5 0.2 setosa 7.7 3.8 6.7 2.2 virginica 6 2.2 4 1 versicolor 7.2 3.2 6 1.8 virginica 5.9 3.2 4.8 1.8 versicolor 6.3 2.7 4.9 1.8 virginica 6.3 3.3 4.7 1.6 versicolor 6.2 3.4 5.4 2.3 virginica 5.4 3.4 1.5 0.4 setosa 5.2 3.5 1.5 0.2 setosa 4.4 3.2 1.3 0.2 setosa 6.1 2.6 5.6 1.4 virginica 5.1 3.8 1.9 0.4 setosa 5.7 3 4.2 1.2 versicolor 5.4 3.4 1.7 0.2 setosa 4.9 3.1 1.5 0.2 setosa 6 3.4 4.5 1.6 versicolor 6.1 2.8 4.7 1.2 versicolor 5.8 2.7 5.1 1.9 virginica 4.8 3.4 1.6 0.2 setosa 6.1 3 4.9 1.8 virginica 6.5 3 5.8 2.2 virginica 5.7 2.9 4.2 1.3 versicolor 6.8 2.8 4.8 1.4 versicolor 5.1 3.3 1.7 0.5 setosa Sampel data untuk di tes \u00b6 python test = [3,5,2,4] print(\"sampel data: \", test) sampel data: [3, 5, 2, 4] Identifikasi Per Grup Class Target untuk data \u00b6 python dataset_classes = {} # table per classes for key,group in dataset.groupby('class'): mu_s = [group[c].mean() for c in group.columns[:-1]] sigma_s = [group[c].std() for c in group.columns[:-1]] dataset_classes[key] = [group, mu_s, sigma_s] print(key, \"===>\") print('Mu_s =>', array(mu_s)) print('Sigma_s =>', array(sigma_s)) table(group) setosa ===> Mu_s => [5.03636364 3.39090909 1.56363636 0.26363636] Sigma_s => [0.33248377 0.221154 0.15666989 0.11200649] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5.4 3.7 1.5 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.1 3.4 1.5 0.2 setosa 5.4 3.4 1.5 0.4 setosa 5.2 3.5 1.5 0.2 setosa 4.4 3.2 1.3 0.2 setosa 5.1 3.8 1.9 0.4 setosa 5.4 3.4 1.7 0.2 setosa 4.9 3.1 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 5.1 3.3 1.7 0.5 setosa versicolor ===> Mu_s => [6.12222222 2.95555556 4.47777778 1.38888889] Sigma_s => [0.38005848 0.35394601 0.29486343 0.24720662] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 6.6 3 4.4 1.4 versicolor 6 2.2 4 1 versicolor 5.9 3.2 4.8 1.8 versicolor 6.3 3.3 4.7 1.6 versicolor 5.7 3 4.2 1.2 versicolor 6 3.4 4.5 1.6 versicolor 6.1 2.8 4.7 1.2 versicolor 5.7 2.9 4.2 1.3 versicolor 6.8 2.8 4.8 1.4 versicolor virginica ===> Mu_s => [6.65 3.01 5.67 1.98] Sigma_s => [0.68677993 0.38715486 0.69610025 0.28982753] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 7.7 2.6 6.9 2.3 virginica 6.9 3.1 5.4 2.1 virginica 7.7 3.8 6.7 2.2 virginica 7.2 3.2 6 1.8 virginica 6.3 2.7 4.9 1.8 virginica 6.2 3.4 5.4 2.3 virginica 6.1 2.6 5.6 1.4 virginica 5.8 2.7 5.1 1.9 virginica 6.1 3 4.9 1.8 virginica 6.5 3 5.8 2.2 virginica Hitung Probabilitas Prior dan Likehood \u00b6 ```python def numericalPriorProbability(v, mu, sigma): return (1.0/sqrt(2 * pi * (sigma 2))*exp(-((v-mu) 2)/(2 (sigma *2)))) def categoricalProbability(sample,universe): return sample.shape[0]/universe.shape[0] Ps = ([[y]+[numericalPriorProbability(x, d[1][i], d[2][i]) for i,x in enumerate(test)]+ [categoricalProbability(d[0],dataset)] for y,d in dataset_classes.items()]) table(DataFrame(Ps, columns=[\"classes\"]+[\"P( %d | C )\" % d for d in test]+[\"P( C )\"])) ``` classes P( 3 | C ) P( 5 | C ) P( 2 | C ) P( 4 | C ) P( C ) setosa 8.58066e-09 5.76458e-12 0.0526488 8.1872e-242 0.366667 versicolor 2.32391e-15 6.41354e-08 6.27963e-16 9.58684e-25 0.3 virginica 4.27211e-07 1.88775e-06 5.27627e-07 3.89569e-11 0.333333 Kesimpulan \u00b6 python Pss = ([[r[0], prod(r[1:])] for r in Ps]) PDss = DataFrame(Pss, columns=['class', 'probability']).sort_values('probability')[::-1] table(PDss) class probability virginica 5.52556e-30 versicolor 2.69183e-62 setosa 7.81778e-263 python print(\"Prediksi Bayes untuk\", test, \"adalah\", PDss.values[0,0]) Prediksi Bayes untuk [3, 5, 2, 4] adalah virginica ## Naive Bayes untuk Data Iris ```python def predict(sampel): priorLikehoods = ([[y]+[numericalPriorProbability(x, d[1][i], d[2][i]) for i,x in enumerate(sampel)]+ [categoricalProbability(d[0],dataset)] for y,d in dataset_classes.items()]) products = ([[r[0], prod(r[1:])] for r in priorLikehoods]) result = DataFrame(products, columns=['class', 'probability']).sort_values('probability')[::-1] return result.values[0,0] dataset_test = DataFrame([list(d)+[predict(d[:4])] for d in data], columns=list(dataset.columns)+['predicted class (by predict())']) table(dataset_test) ``` sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class predicted class (by predict()) 5.1 3.5 1.4 0.2 setosa setosa 4.9 3 1.4 0.2 setosa setosa 4.7 3.2 1.3 0.2 setosa setosa 4.6 3.1 1.5 0.2 setosa setosa 5 3.6 1.4 0.2 setosa setosa 5.4 3.9 1.7 0.4 setosa setosa 4.6 3.4 1.4 0.3 setosa setosa 5 3.4 1.5 0.2 setosa setosa 4.4 2.9 1.4 0.2 setosa setosa 4.9 3.1 1.5 0.1 setosa setosa 5.4 3.7 1.5 0.2 setosa setosa 4.8 3.4 1.6 0.2 setosa setosa 4.8 3 1.4 0.1 setosa setosa 4.3 3 1.1 0.1 setosa setosa 5.8 4 1.2 0.2 setosa setosa 5.7 4.4 1.5 0.4 setosa setosa 5.4 3.9 1.3 0.4 setosa setosa 5.1 3.5 1.4 0.3 setosa setosa 5.7 3.8 1.7 0.3 setosa setosa 5.1 3.8 1.5 0.3 setosa setosa 5.4 3.4 1.7 0.2 setosa setosa 5.1 3.7 1.5 0.4 setosa setosa 4.6 3.6 1 0.2 setosa setosa 5.1 3.3 1.7 0.5 setosa setosa 4.8 3.4 1.9 0.2 setosa setosa 5 3 1.6 0.2 setosa setosa 5 3.4 1.6 0.4 setosa setosa 5.2 3.5 1.5 0.2 setosa setosa 5.2 3.4 1.4 0.2 setosa setosa 4.7 3.2 1.6 0.2 setosa setosa 4.8 3.1 1.6 0.2 setosa setosa 5.4 3.4 1.5 0.4 setosa setosa 5.2 4.1 1.5 0.1 setosa setosa 5.5 4.2 1.4 0.2 setosa setosa 4.9 3.1 1.5 0.2 setosa setosa 5 3.2 1.2 0.2 setosa setosa 5.5 3.5 1.3 0.2 setosa setosa 4.9 3.6 1.4 0.1 setosa setosa 4.4 3 1.3 0.2 setosa setosa 5.1 3.4 1.5 0.2 setosa setosa 5 3.5 1.3 0.3 setosa setosa 4.5 2.3 1.3 0.3 setosa setosa 4.4 3.2 1.3 0.2 setosa setosa 5 3.5 1.6 0.6 setosa setosa 5.1 3.8 1.9 0.4 setosa setosa 4.8 3 1.4 0.3 setosa setosa 5.1 3.8 1.6 0.2 setosa setosa 4.6 3.2 1.4 0.2 setosa setosa 5.3 3.7 1.5 0.2 setosa setosa 5 3.3 1.4 0.2 setosa setosa 7 3.2 4.7 1.4 versicolor versicolor 6.4 3.2 4.5 1.5 versicolor versicolor 6.9 3.1 4.9 1.5 versicolor versicolor 5.5 2.3 4 1.3 versicolor versicolor 6.5 2.8 4.6 1.5 versicolor versicolor 5.7 2.8 4.5 1.3 versicolor versicolor 6.3 3.3 4.7 1.6 versicolor versicolor 4.9 2.4 3.3 1 versicolor versicolor 6.6 2.9 4.6 1.3 versicolor versicolor 5.2 2.7 3.9 1.4 versicolor versicolor 5 2 3.5 1 versicolor versicolor 5.9 3 4.2 1.5 versicolor versicolor 6 2.2 4 1 versicolor versicolor 6.1 2.9 4.7 1.4 versicolor versicolor 5.6 2.9 3.6 1.3 versicolor versicolor 6.7 3.1 4.4 1.4 versicolor versicolor 5.6 3 4.5 1.5 versicolor versicolor 5.8 2.7 4.1 1 versicolor versicolor 6.2 2.2 4.5 1.5 versicolor versicolor 5.6 2.5 3.9 1.1 versicolor versicolor 5.9 3.2 4.8 1.8 versicolor versicolor 6.1 2.8 4 1.3 versicolor versicolor 6.3 2.5 4.9 1.5 versicolor versicolor 6.1 2.8 4.7 1.2 versicolor versicolor 6.4 2.9 4.3 1.3 versicolor versicolor 6.6 3 4.4 1.4 versicolor versicolor 6.8 2.8 4.8 1.4 versicolor versicolor 6.7 3 5 1.7 versicolor virginica 6 2.9 4.5 1.5 versicolor versicolor 5.7 2.6 3.5 1 versicolor versicolor 5.5 2.4 3.8 1.1 versicolor versicolor 5.5 2.4 3.7 1 versicolor versicolor 5.8 2.7 3.9 1.2 versicolor versicolor 6 2.7 5.1 1.6 versicolor versicolor 5.4 3 4.5 1.5 versicolor versicolor 6 3.4 4.5 1.6 versicolor versicolor 6.7 3.1 4.7 1.5 versicolor versicolor 6.3 2.3 4.4 1.3 versicolor versicolor 5.6 3 4.1 1.3 versicolor versicolor 5.5 2.5 4 1.3 versicolor versicolor 5.5 2.6 4.4 1.2 versicolor versicolor 6.1 3 4.6 1.4 versicolor versicolor 5.8 2.6 4 1.2 versicolor versicolor 5 2.3 3.3 1 versicolor versicolor 5.6 2.7 4.2 1.3 versicolor versicolor 5.7 3 4.2 1.2 versicolor versicolor 5.7 2.9 4.2 1.3 versicolor versicolor 6.2 2.9 4.3 1.3 versicolor versicolor 5.1 2.5 3 1.1 versicolor virginica 5.7 2.8 4.1 1.3 versicolor versicolor 6.3 3.3 6 2.5 virginica virginica 5.8 2.7 5.1 1.9 virginica virginica 7.1 3 5.9 2.1 virginica virginica 6.3 2.9 5.6 1.8 virginica virginica 6.5 3 5.8 2.2 virginica virginica 7.6 3 6.6 2.1 virginica virginica 4.9 2.5 4.5 1.7 virginica versicolor 7.3 2.9 6.3 1.8 virginica virginica 6.7 2.5 5.8 1.8 virginica virginica 7.2 3.6 6.1 2.5 virginica virginica 6.5 3.2 5.1 2 virginica virginica 6.4 2.7 5.3 1.9 virginica virginica 6.8 3 5.5 2.1 virginica virginica 5.7 2.5 5 2 virginica virginica 5.8 2.8 5.1 2.4 virginica virginica 6.4 3.2 5.3 2.3 virginica virginica 6.5 3 5.5 1.8 virginica virginica 7.7 3.8 6.7 2.2 virginica virginica 7.7 2.6 6.9 2.3 virginica virginica 6 2.2 5 1.5 virginica versicolor 6.9 3.2 5.7 2.3 virginica virginica 5.6 2.8 4.9 2 virginica virginica 7.7 2.8 6.7 2 virginica virginica 6.3 2.7 4.9 1.8 virginica versicolor 6.7 3.3 5.7 2.1 virginica virginica 7.2 3.2 6 1.8 virginica virginica 6.2 2.8 4.8 1.8 virginica versicolor 6.1 3 4.9 1.8 virginica versicolor 6.4 2.8 5.6 2.1 virginica virginica 7.2 3 5.8 1.6 virginica virginica 7.4 2.8 6.1 1.9 virginica virginica 7.9 3.8 6.4 2 virginica virginica 6.4 2.8 5.6 2.2 virginica virginica 6.3 2.8 5.1 1.5 virginica versicolor 6.1 2.6 5.6 1.4 virginica virginica 7.7 3 6.1 2.3 virginica virginica 6.3 3.4 5.6 2.4 virginica virginica 6.4 3.1 5.5 1.8 virginica virginica 6 3 4.8 1.8 virginica versicolor 6.9 3.1 5.4 2.1 virginica virginica 6.7 3.1 5.6 2.4 virginica virginica 6.9 3.1 5.1 2.3 virginica virginica 5.8 2.7 5.1 1.9 virginica virginica 6.8 3.2 5.9 2.3 virginica virginica 6.7 3.3 5.7 2.5 virginica virginica 6.7 3 5.2 2.3 virginica virginica 6.3 2.5 5 1.9 virginica virginica 6.5 3 5.2 2 virginica virginica 6.2 3.4 5.4 2.3 virginica virginica 5.9 3 5.1 1.8 virginica virginica python corrects = dataset_test.loc[dataset_test['class'] == dataset_test['predicted class (by predict())']].shape[0] print('Prediksi Training Bayes: %d of %d == %f %%' % (corrects, len(data), corrects / len(data) * 100)) Prediksi Training Bayes: 141 of 150 == 94.000000 % Refrensi : https://id.wikipedia.org/wiki/Naive_Bayes_classifier https://www.geeksforgeeks.org/naive-bayes-classifiers/ MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Tugas 4 - Naive Bayes Classifier"},{"location":"naive bayes classifier/#naive-bayes-classifier","text":"Naive Bayes classifier (NBC) merupakan salah satu metoda pemelajaran mesin yang memanfaatkan perhitungan probabilitas dan statistik yang dikemukakan oleh ilmuwan Inggris Thomas Bayes , yaitu memprediksi probabilitas di masa depan berdasarkan pengalaman di masa sebelumnya. Keuntungan penggunan adalah bahwa metoda ini hanya membutuhkan jumlah data pelatihan ( training data ) yg kecil unt menentukan estimasi parameter yg diperlukan dalam proses pengklasifikasian. Karena yg diasumsikan sebagai variable independent, maka hanya varians dr suatu variable dalam sebuah kelas yg dibutuhkan unt menentukan klasifikasi, bukan keseluruhan dr matriks kovarians. Dasar teori naive bayes yang di pakai dalam pemrograman adalah rumus Bayes: $$ P(A|B) = \\frac{(P(B|A)P(A))}{P(B)} $$ Peluang Kejadian A sebagai B di tentukan dari peluang B saat A , peluang A, dan peluang B.Pada pengaplikasiannya nanti rumus ini berubah menjadi $$ P(\\operatorname{C_i|D}) = \\frac{\\left( P(D|C_i)\\right)P(C_i)}{P(D)} $$ Naive Bayes Classifier atau bisa di sebut sebagai multinomina naive bayes merupakan model penyederhanaan dari algoritma bayes yang cocok dalam pengklasifikasian text atau dokumen. P adalah probabilitas yang muncul. Untuk data numerik P adalah: $$ P(x=v|C_k) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_k}}\\exp\\left(-\\frac{(v-\\mu_k)^2}{2\\sigma^2_k}\\right) $$ dimana V adalah nilai dalam fitur.","title":"Naive Bayes Classifier"},{"location":"naive bayes classifier/#tahapan-proses","text":"","title":"Tahapan Proses"},{"location":"naive bayes classifier/#ambil-data-set","text":"```python from sklearn import datasets from pandas import * from numpy import * from math import * from IPython.display import HTML, display; from tabulate import tabulate def table(df): display(HTML(tabulate(df, tablefmt='html', headers='keys', showindex=False))) ``` python iris = datasets.load_iris() data = [list(s)+[iris.target_names[iris.target[i]]] for i,s in enumerate(iris.data)] dataset = DataFrame(data, columns=iris.feature_names+['class']).sample(frac=0.2) table(dataset) sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 7.7 2.6 6.9 2.3 virginica 5.4 3.7 1.5 0.2 setosa 6.9 3.1 5.4 2.1 virginica 6.6 3 4.4 1.4 versicolor 4.6 3.1 1.5 0.2 setosa 5.1 3.4 1.5 0.2 setosa 7.7 3.8 6.7 2.2 virginica 6 2.2 4 1 versicolor 7.2 3.2 6 1.8 virginica 5.9 3.2 4.8 1.8 versicolor 6.3 2.7 4.9 1.8 virginica 6.3 3.3 4.7 1.6 versicolor 6.2 3.4 5.4 2.3 virginica 5.4 3.4 1.5 0.4 setosa 5.2 3.5 1.5 0.2 setosa 4.4 3.2 1.3 0.2 setosa 6.1 2.6 5.6 1.4 virginica 5.1 3.8 1.9 0.4 setosa 5.7 3 4.2 1.2 versicolor 5.4 3.4 1.7 0.2 setosa 4.9 3.1 1.5 0.2 setosa 6 3.4 4.5 1.6 versicolor 6.1 2.8 4.7 1.2 versicolor 5.8 2.7 5.1 1.9 virginica 4.8 3.4 1.6 0.2 setosa 6.1 3 4.9 1.8 virginica 6.5 3 5.8 2.2 virginica 5.7 2.9 4.2 1.3 versicolor 6.8 2.8 4.8 1.4 versicolor 5.1 3.3 1.7 0.5 setosa","title":"Ambil data Set"},{"location":"naive bayes classifier/#sampel-data-untuk-di-tes","text":"python test = [3,5,2,4] print(\"sampel data: \", test) sampel data: [3, 5, 2, 4]","title":"Sampel data untuk di tes"},{"location":"naive bayes classifier/#identifikasi-per-grup-class-target-untuk-data","text":"python dataset_classes = {} # table per classes for key,group in dataset.groupby('class'): mu_s = [group[c].mean() for c in group.columns[:-1]] sigma_s = [group[c].std() for c in group.columns[:-1]] dataset_classes[key] = [group, mu_s, sigma_s] print(key, \"===>\") print('Mu_s =>', array(mu_s)) print('Sigma_s =>', array(sigma_s)) table(group) setosa ===> Mu_s => [5.03636364 3.39090909 1.56363636 0.26363636] Sigma_s => [0.33248377 0.221154 0.15666989 0.11200649] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5.4 3.7 1.5 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.1 3.4 1.5 0.2 setosa 5.4 3.4 1.5 0.4 setosa 5.2 3.5 1.5 0.2 setosa 4.4 3.2 1.3 0.2 setosa 5.1 3.8 1.9 0.4 setosa 5.4 3.4 1.7 0.2 setosa 4.9 3.1 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 5.1 3.3 1.7 0.5 setosa versicolor ===> Mu_s => [6.12222222 2.95555556 4.47777778 1.38888889] Sigma_s => [0.38005848 0.35394601 0.29486343 0.24720662] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 6.6 3 4.4 1.4 versicolor 6 2.2 4 1 versicolor 5.9 3.2 4.8 1.8 versicolor 6.3 3.3 4.7 1.6 versicolor 5.7 3 4.2 1.2 versicolor 6 3.4 4.5 1.6 versicolor 6.1 2.8 4.7 1.2 versicolor 5.7 2.9 4.2 1.3 versicolor 6.8 2.8 4.8 1.4 versicolor virginica ===> Mu_s => [6.65 3.01 5.67 1.98] Sigma_s => [0.68677993 0.38715486 0.69610025 0.28982753] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 7.7 2.6 6.9 2.3 virginica 6.9 3.1 5.4 2.1 virginica 7.7 3.8 6.7 2.2 virginica 7.2 3.2 6 1.8 virginica 6.3 2.7 4.9 1.8 virginica 6.2 3.4 5.4 2.3 virginica 6.1 2.6 5.6 1.4 virginica 5.8 2.7 5.1 1.9 virginica 6.1 3 4.9 1.8 virginica 6.5 3 5.8 2.2 virginica","title":"Identifikasi Per Grup Class Target untuk data"},{"location":"naive bayes classifier/#hitung-probabilitas-prior-dan-likehood","text":"```python def numericalPriorProbability(v, mu, sigma): return (1.0/sqrt(2 * pi * (sigma 2))*exp(-((v-mu) 2)/(2 (sigma *2)))) def categoricalProbability(sample,universe): return sample.shape[0]/universe.shape[0] Ps = ([[y]+[numericalPriorProbability(x, d[1][i], d[2][i]) for i,x in enumerate(test)]+ [categoricalProbability(d[0],dataset)] for y,d in dataset_classes.items()]) table(DataFrame(Ps, columns=[\"classes\"]+[\"P( %d | C )\" % d for d in test]+[\"P( C )\"])) ``` classes P( 3 | C ) P( 5 | C ) P( 2 | C ) P( 4 | C ) P( C ) setosa 8.58066e-09 5.76458e-12 0.0526488 8.1872e-242 0.366667 versicolor 2.32391e-15 6.41354e-08 6.27963e-16 9.58684e-25 0.3 virginica 4.27211e-07 1.88775e-06 5.27627e-07 3.89569e-11 0.333333","title":"Hitung Probabilitas Prior dan Likehood"},{"location":"naive bayes classifier/#kesimpulan","text":"python Pss = ([[r[0], prod(r[1:])] for r in Ps]) PDss = DataFrame(Pss, columns=['class', 'probability']).sort_values('probability')[::-1] table(PDss) class probability virginica 5.52556e-30 versicolor 2.69183e-62 setosa 7.81778e-263 python print(\"Prediksi Bayes untuk\", test, \"adalah\", PDss.values[0,0]) Prediksi Bayes untuk [3, 5, 2, 4] adalah virginica ## Naive Bayes untuk Data Iris ```python def predict(sampel): priorLikehoods = ([[y]+[numericalPriorProbability(x, d[1][i], d[2][i]) for i,x in enumerate(sampel)]+ [categoricalProbability(d[0],dataset)] for y,d in dataset_classes.items()]) products = ([[r[0], prod(r[1:])] for r in priorLikehoods]) result = DataFrame(products, columns=['class', 'probability']).sort_values('probability')[::-1] return result.values[0,0] dataset_test = DataFrame([list(d)+[predict(d[:4])] for d in data], columns=list(dataset.columns)+['predicted class (by predict())']) table(dataset_test) ``` sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class predicted class (by predict()) 5.1 3.5 1.4 0.2 setosa setosa 4.9 3 1.4 0.2 setosa setosa 4.7 3.2 1.3 0.2 setosa setosa 4.6 3.1 1.5 0.2 setosa setosa 5 3.6 1.4 0.2 setosa setosa 5.4 3.9 1.7 0.4 setosa setosa 4.6 3.4 1.4 0.3 setosa setosa 5 3.4 1.5 0.2 setosa setosa 4.4 2.9 1.4 0.2 setosa setosa 4.9 3.1 1.5 0.1 setosa setosa 5.4 3.7 1.5 0.2 setosa setosa 4.8 3.4 1.6 0.2 setosa setosa 4.8 3 1.4 0.1 setosa setosa 4.3 3 1.1 0.1 setosa setosa 5.8 4 1.2 0.2 setosa setosa 5.7 4.4 1.5 0.4 setosa setosa 5.4 3.9 1.3 0.4 setosa setosa 5.1 3.5 1.4 0.3 setosa setosa 5.7 3.8 1.7 0.3 setosa setosa 5.1 3.8 1.5 0.3 setosa setosa 5.4 3.4 1.7 0.2 setosa setosa 5.1 3.7 1.5 0.4 setosa setosa 4.6 3.6 1 0.2 setosa setosa 5.1 3.3 1.7 0.5 setosa setosa 4.8 3.4 1.9 0.2 setosa setosa 5 3 1.6 0.2 setosa setosa 5 3.4 1.6 0.4 setosa setosa 5.2 3.5 1.5 0.2 setosa setosa 5.2 3.4 1.4 0.2 setosa setosa 4.7 3.2 1.6 0.2 setosa setosa 4.8 3.1 1.6 0.2 setosa setosa 5.4 3.4 1.5 0.4 setosa setosa 5.2 4.1 1.5 0.1 setosa setosa 5.5 4.2 1.4 0.2 setosa setosa 4.9 3.1 1.5 0.2 setosa setosa 5 3.2 1.2 0.2 setosa setosa 5.5 3.5 1.3 0.2 setosa setosa 4.9 3.6 1.4 0.1 setosa setosa 4.4 3 1.3 0.2 setosa setosa 5.1 3.4 1.5 0.2 setosa setosa 5 3.5 1.3 0.3 setosa setosa 4.5 2.3 1.3 0.3 setosa setosa 4.4 3.2 1.3 0.2 setosa setosa 5 3.5 1.6 0.6 setosa setosa 5.1 3.8 1.9 0.4 setosa setosa 4.8 3 1.4 0.3 setosa setosa 5.1 3.8 1.6 0.2 setosa setosa 4.6 3.2 1.4 0.2 setosa setosa 5.3 3.7 1.5 0.2 setosa setosa 5 3.3 1.4 0.2 setosa setosa 7 3.2 4.7 1.4 versicolor versicolor 6.4 3.2 4.5 1.5 versicolor versicolor 6.9 3.1 4.9 1.5 versicolor versicolor 5.5 2.3 4 1.3 versicolor versicolor 6.5 2.8 4.6 1.5 versicolor versicolor 5.7 2.8 4.5 1.3 versicolor versicolor 6.3 3.3 4.7 1.6 versicolor versicolor 4.9 2.4 3.3 1 versicolor versicolor 6.6 2.9 4.6 1.3 versicolor versicolor 5.2 2.7 3.9 1.4 versicolor versicolor 5 2 3.5 1 versicolor versicolor 5.9 3 4.2 1.5 versicolor versicolor 6 2.2 4 1 versicolor versicolor 6.1 2.9 4.7 1.4 versicolor versicolor 5.6 2.9 3.6 1.3 versicolor versicolor 6.7 3.1 4.4 1.4 versicolor versicolor 5.6 3 4.5 1.5 versicolor versicolor 5.8 2.7 4.1 1 versicolor versicolor 6.2 2.2 4.5 1.5 versicolor versicolor 5.6 2.5 3.9 1.1 versicolor versicolor 5.9 3.2 4.8 1.8 versicolor versicolor 6.1 2.8 4 1.3 versicolor versicolor 6.3 2.5 4.9 1.5 versicolor versicolor 6.1 2.8 4.7 1.2 versicolor versicolor 6.4 2.9 4.3 1.3 versicolor versicolor 6.6 3 4.4 1.4 versicolor versicolor 6.8 2.8 4.8 1.4 versicolor versicolor 6.7 3 5 1.7 versicolor virginica 6 2.9 4.5 1.5 versicolor versicolor 5.7 2.6 3.5 1 versicolor versicolor 5.5 2.4 3.8 1.1 versicolor versicolor 5.5 2.4 3.7 1 versicolor versicolor 5.8 2.7 3.9 1.2 versicolor versicolor 6 2.7 5.1 1.6 versicolor versicolor 5.4 3 4.5 1.5 versicolor versicolor 6 3.4 4.5 1.6 versicolor versicolor 6.7 3.1 4.7 1.5 versicolor versicolor 6.3 2.3 4.4 1.3 versicolor versicolor 5.6 3 4.1 1.3 versicolor versicolor 5.5 2.5 4 1.3 versicolor versicolor 5.5 2.6 4.4 1.2 versicolor versicolor 6.1 3 4.6 1.4 versicolor versicolor 5.8 2.6 4 1.2 versicolor versicolor 5 2.3 3.3 1 versicolor versicolor 5.6 2.7 4.2 1.3 versicolor versicolor 5.7 3 4.2 1.2 versicolor versicolor 5.7 2.9 4.2 1.3 versicolor versicolor 6.2 2.9 4.3 1.3 versicolor versicolor 5.1 2.5 3 1.1 versicolor virginica 5.7 2.8 4.1 1.3 versicolor versicolor 6.3 3.3 6 2.5 virginica virginica 5.8 2.7 5.1 1.9 virginica virginica 7.1 3 5.9 2.1 virginica virginica 6.3 2.9 5.6 1.8 virginica virginica 6.5 3 5.8 2.2 virginica virginica 7.6 3 6.6 2.1 virginica virginica 4.9 2.5 4.5 1.7 virginica versicolor 7.3 2.9 6.3 1.8 virginica virginica 6.7 2.5 5.8 1.8 virginica virginica 7.2 3.6 6.1 2.5 virginica virginica 6.5 3.2 5.1 2 virginica virginica 6.4 2.7 5.3 1.9 virginica virginica 6.8 3 5.5 2.1 virginica virginica 5.7 2.5 5 2 virginica virginica 5.8 2.8 5.1 2.4 virginica virginica 6.4 3.2 5.3 2.3 virginica virginica 6.5 3 5.5 1.8 virginica virginica 7.7 3.8 6.7 2.2 virginica virginica 7.7 2.6 6.9 2.3 virginica virginica 6 2.2 5 1.5 virginica versicolor 6.9 3.2 5.7 2.3 virginica virginica 5.6 2.8 4.9 2 virginica virginica 7.7 2.8 6.7 2 virginica virginica 6.3 2.7 4.9 1.8 virginica versicolor 6.7 3.3 5.7 2.1 virginica virginica 7.2 3.2 6 1.8 virginica virginica 6.2 2.8 4.8 1.8 virginica versicolor 6.1 3 4.9 1.8 virginica versicolor 6.4 2.8 5.6 2.1 virginica virginica 7.2 3 5.8 1.6 virginica virginica 7.4 2.8 6.1 1.9 virginica virginica 7.9 3.8 6.4 2 virginica virginica 6.4 2.8 5.6 2.2 virginica virginica 6.3 2.8 5.1 1.5 virginica versicolor 6.1 2.6 5.6 1.4 virginica virginica 7.7 3 6.1 2.3 virginica virginica 6.3 3.4 5.6 2.4 virginica virginica 6.4 3.1 5.5 1.8 virginica virginica 6 3 4.8 1.8 virginica versicolor 6.9 3.1 5.4 2.1 virginica virginica 6.7 3.1 5.6 2.4 virginica virginica 6.9 3.1 5.1 2.3 virginica virginica 5.8 2.7 5.1 1.9 virginica virginica 6.8 3.2 5.9 2.3 virginica virginica 6.7 3.3 5.7 2.5 virginica virginica 6.7 3 5.2 2.3 virginica virginica 6.3 2.5 5 1.9 virginica virginica 6.5 3 5.2 2 virginica virginica 6.2 3.4 5.4 2.3 virginica virginica 5.9 3 5.1 1.8 virginica virginica python corrects = dataset_test.loc[dataset_test['class'] == dataset_test['predicted class (by predict())']].shape[0] print('Prediksi Training Bayes: %d of %d == %f %%' % (corrects, len(data), corrects / len(data) * 100)) Prediksi Training Bayes: 141 of 150 == 94.000000 % Refrensi : https://id.wikipedia.org/wiki/Naive_Bayes_classifier https://www.geeksforgeeks.org/naive-bayes-classifiers/ MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Kesimpulan"},{"location":"statistik deskriptif/","text":"Pengertian \u00b6 Statistika deskriptif adalah metode-metode yang berkaitan dengan pengumpulan dan penyajian suatu gugus data sehingga memberikan informasi yang berguna. Pengklasifikasian menjadi statistika deskriptif dan statistika inferensi dilakukan berdasarkan aktivitas yang dilakukan. Statistika deskriptif hanya memberikan informasi mengenai data yang dipunyai dan sama sekali tidak menarik inferensia atau kesimpulan apapun tentang gugus induknya yang lebih besar. Contoh statistika deskriptif yang sering muncul adalah, tabel, diagram, grafik, dan besaran-besaran lain di majalah dan koran-koran Dengan Statistika deskriptif, kumpulan data yang diperoleh akan tersaji dengan ringkas dan rapi serta dapat memberikan informasi inti dari kumpulan data yang ada. Informasi yang dapat diperoleh dari statistika deskriptif ini antara lain: ukuran pemusatan data, ukuran penyebaran data serta kecenderungan suatu gugus data. Tipe Statistik Deskriptif \u00b6 Mean (rata-rata) \u00b6 Mean adalah *nilai rata-rata* dari beberapa buah data. Nilai mean dapat ditentukan dengan membagi jumlah data dengan banyaknya data. Mean (rata-rata) merupakan suatu ukuran pemusatan data. Mean suatu data juga merupakan statistik karena mampu menggambarkan bahwa data tersebut berada pada kisaran mean data tersebut. Mean tidak dapat digunakan sebagai ukuran pemusatan untuk jenis data nominal dan ordinal. Berdasarkan definisi dari mean adalah jumlah seluruh data dibagi dengan banyaknya data. Dengan kata lain jika kita memiliki N data sebagai berikut maka mean data tersebut dapat kita tuliskan sebagai berikut : $$ \\bar x ={\\sum \\limits_{i=1}^{n} x_i \\over N} = {x_1 + x_2 + x_3 + ... + x_n \\over N} $$ Dimana: x = data ke n x bar = x rata-rata = nilai rata-rata sampel n = banyaknya data Median \u00b6 Median menentukan letak tengah data setelah data disusun menurut urutan nilainya. Bisa juga *nilai tengah dari data-data yang terurut. Simbol untuk median adalah Me. Dengan median Me, maka 50% dari banyak data nilainya paling tinggi sama dengan Me, dan 50% dari banyak data nilainya paling rendah sama dengan Me. Dalam mencari median, dibedakan untuk banyak data ganjil dan banyak data genap. Untuk banyak data ganjil, setelah data disusun menurut nilainya, maka median Me adalah data yang terletak tepat di tengah. Median bisa dihitung menggunakan rumus sebagai berikut: $$ Me=Q_2 =\\left( \\begin{matrix} ^xn+1 \\over 2 \\end{matrix} \\right), jika n ganjil $$ $$ Me=Q_2 =\\left( \\begin{matrix} {^xn \\over 2 } {^xn+1\\over 2} \\over 2 \\end{matrix} \\right), jika n genap $$ Modus \u00b6 Modus adalah nilai yang sering muncul. Jika kita tertarik pada data frekuensi, jumlah dari suatu nilai dari kumpulan data, maka kita menggunakan modus. Modus sangat baik bila digunakan untuk data yang memiliki sekala kategorik yaitu nominal atau ordinal. Modus bisa dihitung menggunakan rumus sebagai berikut: $$ M_o = L + i{b_i \\over b_1 + b_2} $$ Dimana: Mo = Modus L = Tepi bawah kelas yang memiliki frekuensi tertinggi (kelas modus) i = Interval kelas b1 = Frekuensi kelas modus dikurangi frekuensi kelas interval terdekat sebelumnya b2 = frekuensi kelas modus dikurangi frekuensi kelas interval terdekat sesudahnya Standar Deviasi \u00b6 Standar Deviasi dan Varians Salah satu teknik statistik yg digunakan untuk menjelaskan homogenitas kelompok. Varians merupakan jumlah kuadrat semua deviasi nilai-nilai individual terhadap rata-rata kelompok. Sedangkan akar dari varians disebut dengan standar deviasi atau simpangan baku. Standar Deviasi dan Varians Simpangan baku merupakan variasi sebaran data. Semakin kecil nilai sebarannya berarti variasi nilai data makin sama Jika sebarannya bernilai 0, maka nilai semua datanya adalah sama. Semakin besar nilai sebarannya berarti data semakin bervariasi. Standar Deviasi bisa didapat menggunakan rumus sebagai berikut: $$ \\sigma = {\\sqrt{ \\sum \\limits_{i=1}^{n} {(x_1-\\bar x)^2 } \\over n }} $$ Dimana : x = data ke n x bar = x rata-rata = nilai rata-rata sampel n = banyaknya data Varians \u00b6 Varians merupakan rata-rata dari selisih kuadrat tersebut merupakan suatu ukuran penyimpangan dari observasi. Simbol varians pada ukuran populasi sigma kuarat dan pada ukuran sample S2. Akar dari varians dinamakan standar deviasi atau simpangan baku. Varians bisa didapat menggunakan rumus sebagai berikut: $$ S^2 = {\\sqrt{ \\sum \\limits_{i=1}^{n} {(x_1-\\bar x)^2 } \\over n-1 }} $$ Skewness \u00b6 Skewness ( kemencengan ) adalah derajat ketidaksimetrisan suatu distribusi. Jika kurva frekuensi suatu distribusi memiliki ekor yang lebih memanjang ke kanan (dilihat dari meannya) maka dikatakan menceng kanan (positif) dan jika sebaliknya maka menceng kiri (negatif). Secara perhitungan, skewness adalah momen ketiga terhadap mean. Distribusi normal (dan distribusi simetris lainnya, misalnya distribusi t atau Cauchy) memiliki skewness 0 (nol). Skewness bisa dihitung menggunakan rumus sebagai berikut: $$ Skewness (S) = {{1 \\over T\\sigma^3} \\sum \\limits_{i=1}^{n}(r_2 - \\mu)^3} $$ Quartile \u00b6 Quartile adalah nilai-nilai yang membagi segugus pengamatan menjadi empat bagian sama besar. Nilai-nilai itu, yang dilambangkan dengan Q1, Q2, dan Q3, mempunyai sifat bahwa 25% data jatuh dibawah Q1, 50% data jatuh dibawah Q2, dan 75% data jatuh dibawah Q3. Quartile bisa dihitung menggunakan rumus sebagai berikut: $$ Q_1 = {x_{{1\\over4}(n+1)}} $$ $$ Q_2 = {x_{{1\\over2}(n+1)}} $$ $$ Q_3 = {x_{{3\\over4}(n+1)}} $$ Penerapan statistik deskriptif menggunakan python \u00b6 Alat dan Bahan \u00b6 Pada penerapan ini saya menggunakan 500 data random yang disimpan dalam bentuk .csv dan untuk mempermudah dalam penerapan tersebut, perlu disiapkan library python yang dapat didownload secara gratis. dalam kasus ini library python yang digunakan adalah sebagai berikut: pandas, digunakan untuk data manajemen dan data analysis. scipy, berisi kumpulan algoritme dan fungsi matematika. Pertama \u00b6 pada langkah ini kita memasukkan library yang telah disiapkan sebelumya import pandas as pd from scipy import stats Kedua \u00b6 dan selanjutnya memuat data csv yang telah disiapkan df = pd . read_csv ( 'Data.csv' , sep = ';' ) Ketiga \u00b6 kemudian membuat data penyimpanan ( dictionary ) yang menampung nilai yang akan ditampilkan. selanjutnya mengambil data dari beberapa kolom pada csv dengan cara diiterasi serta menghitungnya dengan berbagai metode yang telah disiapkan oleh pandas itu sendiri. kemudian hasil tersebut di disimpan pada penyimpanan tadi data = { \"Stats\" : [ 'Min' , 'Max' , 'Mean' , 'Standard Deviasi' , 'Variasi' , 'Skewnes' , 'Quartile 1' , 'Quartile 2' , 'Quartile 3' , 'Median' , 'Modus' ]} for i in df . columns : data [ i ] = [ df [ i ] . min (), df [ i ] . max (), df [ i ] . mean (), round ( df [ i ] . std (), 2 ), round ( df [ i ] . var (), 2 ), round ( df [ i ] . skew (), 2 ), df [ i ] . quantile ( 0.25 ), df [ i ] . quantile ( 0.5 ), df [ i ] . quantile ( 0.75 ), df [ i ] . median (), stats . mode ( df [ i ]) . mode [ 0 ]] Keempat \u00b6 terakhir adalah menvisualisasikan hasil tersebut dalam bentuk dataframe tes = pd . DataFrame ( data ) tes . style . hide_index () stats data 1 data 2 data 3 data 4 Min 81 81 80 80 Max 160 160 160 160 Mean 120.26 124.67 122 118.02 Standard Deviasi 24.78 22.47 24.55 22.67 Variasi 613.89 504.89 602.57 513.72 Skewnes 0.11 -0.22 -0.07 0.12 Quantile 1 97 106.75 101.75 99.5 Quantile 2 117.5 126.5 123.5 118 Quantile 3 143.25 143.25 144 137.25 Median 117.5 126.5 123.5 118 Modus 84 131 89 124 Source \u00b6 Seluruh file percobaan ada pada link berikut : disini Referensi http://blog.ub.ac.id/adiarsa/2012/03/14/mean-median-modus-dan-standar-deviasi/ http://statutorial.blogspot.com/2008/01/skewness-dan-kurtosis.html https://www.rumusstatistik.com/2013/07/varian-dan-standar-deviasi-simpangan.html https://www.rumusstatistik.com/2016/12/membuat-rumus-matematika-dengan-latex.html https://englishccit.wordpress.com/2012/03/27/pengertian-statistik-deskriptif/#more-1194 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Tugas 1 - statistik deskriptif"},{"location":"statistik deskriptif/#pengertian","text":"Statistika deskriptif adalah metode-metode yang berkaitan dengan pengumpulan dan penyajian suatu gugus data sehingga memberikan informasi yang berguna. Pengklasifikasian menjadi statistika deskriptif dan statistika inferensi dilakukan berdasarkan aktivitas yang dilakukan. Statistika deskriptif hanya memberikan informasi mengenai data yang dipunyai dan sama sekali tidak menarik inferensia atau kesimpulan apapun tentang gugus induknya yang lebih besar. Contoh statistika deskriptif yang sering muncul adalah, tabel, diagram, grafik, dan besaran-besaran lain di majalah dan koran-koran Dengan Statistika deskriptif, kumpulan data yang diperoleh akan tersaji dengan ringkas dan rapi serta dapat memberikan informasi inti dari kumpulan data yang ada. Informasi yang dapat diperoleh dari statistika deskriptif ini antara lain: ukuran pemusatan data, ukuran penyebaran data serta kecenderungan suatu gugus data.","title":"Pengertian"},{"location":"statistik deskriptif/#tipe-statistik-deskriptif","text":"","title":"Tipe Statistik Deskriptif"},{"location":"statistik deskriptif/#mean-rata-rata","text":"Mean adalah *nilai rata-rata* dari beberapa buah data. Nilai mean dapat ditentukan dengan membagi jumlah data dengan banyaknya data. Mean (rata-rata) merupakan suatu ukuran pemusatan data. Mean suatu data juga merupakan statistik karena mampu menggambarkan bahwa data tersebut berada pada kisaran mean data tersebut. Mean tidak dapat digunakan sebagai ukuran pemusatan untuk jenis data nominal dan ordinal. Berdasarkan definisi dari mean adalah jumlah seluruh data dibagi dengan banyaknya data. Dengan kata lain jika kita memiliki N data sebagai berikut maka mean data tersebut dapat kita tuliskan sebagai berikut : $$ \\bar x ={\\sum \\limits_{i=1}^{n} x_i \\over N} = {x_1 + x_2 + x_3 + ... + x_n \\over N} $$ Dimana: x = data ke n x bar = x rata-rata = nilai rata-rata sampel n = banyaknya data","title":"Mean (rata-rata)"},{"location":"statistik deskriptif/#median","text":"Median menentukan letak tengah data setelah data disusun menurut urutan nilainya. Bisa juga *nilai tengah dari data-data yang terurut. Simbol untuk median adalah Me. Dengan median Me, maka 50% dari banyak data nilainya paling tinggi sama dengan Me, dan 50% dari banyak data nilainya paling rendah sama dengan Me. Dalam mencari median, dibedakan untuk banyak data ganjil dan banyak data genap. Untuk banyak data ganjil, setelah data disusun menurut nilainya, maka median Me adalah data yang terletak tepat di tengah. Median bisa dihitung menggunakan rumus sebagai berikut: $$ Me=Q_2 =\\left( \\begin{matrix} ^xn+1 \\over 2 \\end{matrix} \\right), jika n ganjil $$ $$ Me=Q_2 =\\left( \\begin{matrix} {^xn \\over 2 } {^xn+1\\over 2} \\over 2 \\end{matrix} \\right), jika n genap $$","title":"Median"},{"location":"statistik deskriptif/#modus","text":"Modus adalah nilai yang sering muncul. Jika kita tertarik pada data frekuensi, jumlah dari suatu nilai dari kumpulan data, maka kita menggunakan modus. Modus sangat baik bila digunakan untuk data yang memiliki sekala kategorik yaitu nominal atau ordinal. Modus bisa dihitung menggunakan rumus sebagai berikut: $$ M_o = L + i{b_i \\over b_1 + b_2} $$ Dimana: Mo = Modus L = Tepi bawah kelas yang memiliki frekuensi tertinggi (kelas modus) i = Interval kelas b1 = Frekuensi kelas modus dikurangi frekuensi kelas interval terdekat sebelumnya b2 = frekuensi kelas modus dikurangi frekuensi kelas interval terdekat sesudahnya","title":"Modus"},{"location":"statistik deskriptif/#standar-deviasi","text":"Standar Deviasi dan Varians Salah satu teknik statistik yg digunakan untuk menjelaskan homogenitas kelompok. Varians merupakan jumlah kuadrat semua deviasi nilai-nilai individual terhadap rata-rata kelompok. Sedangkan akar dari varians disebut dengan standar deviasi atau simpangan baku. Standar Deviasi dan Varians Simpangan baku merupakan variasi sebaran data. Semakin kecil nilai sebarannya berarti variasi nilai data makin sama Jika sebarannya bernilai 0, maka nilai semua datanya adalah sama. Semakin besar nilai sebarannya berarti data semakin bervariasi. Standar Deviasi bisa didapat menggunakan rumus sebagai berikut: $$ \\sigma = {\\sqrt{ \\sum \\limits_{i=1}^{n} {(x_1-\\bar x)^2 } \\over n }} $$ Dimana : x = data ke n x bar = x rata-rata = nilai rata-rata sampel n = banyaknya data","title":"Standar Deviasi"},{"location":"statistik deskriptif/#varians","text":"Varians merupakan rata-rata dari selisih kuadrat tersebut merupakan suatu ukuran penyimpangan dari observasi. Simbol varians pada ukuran populasi sigma kuarat dan pada ukuran sample S2. Akar dari varians dinamakan standar deviasi atau simpangan baku. Varians bisa didapat menggunakan rumus sebagai berikut: $$ S^2 = {\\sqrt{ \\sum \\limits_{i=1}^{n} {(x_1-\\bar x)^2 } \\over n-1 }} $$","title":"Varians"},{"location":"statistik deskriptif/#skewness","text":"Skewness ( kemencengan ) adalah derajat ketidaksimetrisan suatu distribusi. Jika kurva frekuensi suatu distribusi memiliki ekor yang lebih memanjang ke kanan (dilihat dari meannya) maka dikatakan menceng kanan (positif) dan jika sebaliknya maka menceng kiri (negatif). Secara perhitungan, skewness adalah momen ketiga terhadap mean. Distribusi normal (dan distribusi simetris lainnya, misalnya distribusi t atau Cauchy) memiliki skewness 0 (nol). Skewness bisa dihitung menggunakan rumus sebagai berikut: $$ Skewness (S) = {{1 \\over T\\sigma^3} \\sum \\limits_{i=1}^{n}(r_2 - \\mu)^3} $$","title":"Skewness"},{"location":"statistik deskriptif/#quartile","text":"Quartile adalah nilai-nilai yang membagi segugus pengamatan menjadi empat bagian sama besar. Nilai-nilai itu, yang dilambangkan dengan Q1, Q2, dan Q3, mempunyai sifat bahwa 25% data jatuh dibawah Q1, 50% data jatuh dibawah Q2, dan 75% data jatuh dibawah Q3. Quartile bisa dihitung menggunakan rumus sebagai berikut: $$ Q_1 = {x_{{1\\over4}(n+1)}} $$ $$ Q_2 = {x_{{1\\over2}(n+1)}} $$ $$ Q_3 = {x_{{3\\over4}(n+1)}} $$","title":"Quartile"},{"location":"statistik deskriptif/#penerapan-statistik-deskriptif-menggunakan-python","text":"","title":"Penerapan statistik deskriptif menggunakan python"},{"location":"statistik deskriptif/#alat-dan-bahan","text":"Pada penerapan ini saya menggunakan 500 data random yang disimpan dalam bentuk .csv dan untuk mempermudah dalam penerapan tersebut, perlu disiapkan library python yang dapat didownload secara gratis. dalam kasus ini library python yang digunakan adalah sebagai berikut: pandas, digunakan untuk data manajemen dan data analysis. scipy, berisi kumpulan algoritme dan fungsi matematika.","title":"Alat dan Bahan"},{"location":"statistik deskriptif/#pertama","text":"pada langkah ini kita memasukkan library yang telah disiapkan sebelumya import pandas as pd from scipy import stats","title":"Pertama"},{"location":"statistik deskriptif/#kedua","text":"dan selanjutnya memuat data csv yang telah disiapkan df = pd . read_csv ( 'Data.csv' , sep = ';' )","title":"Kedua"},{"location":"statistik deskriptif/#ketiga","text":"kemudian membuat data penyimpanan ( dictionary ) yang menampung nilai yang akan ditampilkan. selanjutnya mengambil data dari beberapa kolom pada csv dengan cara diiterasi serta menghitungnya dengan berbagai metode yang telah disiapkan oleh pandas itu sendiri. kemudian hasil tersebut di disimpan pada penyimpanan tadi data = { \"Stats\" : [ 'Min' , 'Max' , 'Mean' , 'Standard Deviasi' , 'Variasi' , 'Skewnes' , 'Quartile 1' , 'Quartile 2' , 'Quartile 3' , 'Median' , 'Modus' ]} for i in df . columns : data [ i ] = [ df [ i ] . min (), df [ i ] . max (), df [ i ] . mean (), round ( df [ i ] . std (), 2 ), round ( df [ i ] . var (), 2 ), round ( df [ i ] . skew (), 2 ), df [ i ] . quantile ( 0.25 ), df [ i ] . quantile ( 0.5 ), df [ i ] . quantile ( 0.75 ), df [ i ] . median (), stats . mode ( df [ i ]) . mode [ 0 ]]","title":"Ketiga"},{"location":"statistik deskriptif/#keempat","text":"terakhir adalah menvisualisasikan hasil tersebut dalam bentuk dataframe tes = pd . DataFrame ( data ) tes . style . hide_index () stats data 1 data 2 data 3 data 4 Min 81 81 80 80 Max 160 160 160 160 Mean 120.26 124.67 122 118.02 Standard Deviasi 24.78 22.47 24.55 22.67 Variasi 613.89 504.89 602.57 513.72 Skewnes 0.11 -0.22 -0.07 0.12 Quantile 1 97 106.75 101.75 99.5 Quantile 2 117.5 126.5 123.5 118 Quantile 3 143.25 143.25 144 137.25 Median 117.5 126.5 123.5 118 Modus 84 131 89 124","title":"Keempat"},{"location":"statistik deskriptif/#source","text":"Seluruh file percobaan ada pada link berikut : disini Referensi http://blog.ub.ac.id/adiarsa/2012/03/14/mean-median-modus-dan-standar-deviasi/ http://statutorial.blogspot.com/2008/01/skewness-dan-kurtosis.html https://www.rumusstatistik.com/2013/07/varian-dan-standar-deviasi-simpangan.html https://www.rumusstatistik.com/2016/12/membuat-rumus-matematika-dengan-latex.html https://englishccit.wordpress.com/2012/03/27/pengertian-statistik-deskriptif/#more-1194 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Source"},{"location":"uts/","text":"UJIAN TENGAH SEMSTER \u00b6 Menentukan Play yes or no dengan Naive Bayes Data \u00b6 outlook temperature humidity windy play sunny 85 high FALSE no sunny 80 high TRUE no overcast 83 high FALSE yes rainy 70 high FALSE yes rainy 68 normal FALSE yes rainy 65 normal TRUE no overcast 64 normal TRUE yes sunny 72 high FALSE no sunny 69 normal FALSE yes rainy 75 normal FALSE yes sunny 75 normal TRUE yes overcast 72 high TRUE yes overcast 81 normal FALSE yes rainy 71 high TRUE no Tentukan \u00b6 outlook temperature humidity windy play rainy 60 high FALSE ? Rumus Mencari Pobability \u00b6 Rumus Mencari Gaussian \u00b6 Mencari Probability dan Gaussian Play no dan yes dari : Outlook = rainy (Probability) temperature = 60 (Gaussian) humidity = high (Probability) windy = FALSE (Probability) Probabilitas \u00b6 p (Ci) p (play | no) = 0,357142857 p (play | yes) = 0,642857143 Play = NO \u00b6 1. P(Outlook=rainy|Play=no) = 2/5 = 0,4 2. P(temperatur=60|Play=no) = 0,060367397 3. P(humidity=high|Play=no) = 4/5 = 0,8 4. P(windy=false|Play=no) = 2/5 = 0,4 Total = 0,007727027 Play = YES \u00b6 1. P(Outlook=rainy|Play=yes) = 3/9 = 0,333333333 2. P(temperatur=60|Play=yes) = 0,015194057 3. P(humidity=high|Play=yes) = 3/9 = 0,333333333 4. P(windy=false|Play=yes) = 6/9 = 0,666666667 Total = 0,001125486 Hasil \u00b6 p(X|Ci)*P(Ci) p(X|play = no)*p(no) = 0,357142857 x 0,007727027 = 0,002759652 p(X|play =yes)*p(yes) = 0,642857143 x 0,001125486 = 0,000723527 Jadi Cari kedua hasil ini yang terbesar , yang terbesar adalah Play = NO outlook temperature humidity windy play rainy 60 high FALSE no [Data UTS dalam Bentuk Excel][https://github.com/bimaikhsan/datamining/blob/master/uts.xlsx]","title":"UTS"},{"location":"uts/#ujian-tengah-semster","text":"Menentukan Play yes or no dengan Naive Bayes","title":"UJIAN TENGAH SEMSTER"},{"location":"uts/#data","text":"outlook temperature humidity windy play sunny 85 high FALSE no sunny 80 high TRUE no overcast 83 high FALSE yes rainy 70 high FALSE yes rainy 68 normal FALSE yes rainy 65 normal TRUE no overcast 64 normal TRUE yes sunny 72 high FALSE no sunny 69 normal FALSE yes rainy 75 normal FALSE yes sunny 75 normal TRUE yes overcast 72 high TRUE yes overcast 81 normal FALSE yes rainy 71 high TRUE no","title":"Data"},{"location":"uts/#tentukan","text":"outlook temperature humidity windy play rainy 60 high FALSE ?","title":"Tentukan"},{"location":"uts/#rumus-mencari-pobability","text":"","title":"Rumus Mencari Pobability"},{"location":"uts/#rumus-mencari-gaussian","text":"Mencari Probability dan Gaussian Play no dan yes dari : Outlook = rainy (Probability) temperature = 60 (Gaussian) humidity = high (Probability) windy = FALSE (Probability)","title":"Rumus Mencari Gaussian"},{"location":"uts/#probabilitas","text":"p (Ci) p (play | no) = 0,357142857 p (play | yes) = 0,642857143","title":"Probabilitas"},{"location":"uts/#play-no","text":"1. P(Outlook=rainy|Play=no) = 2/5 = 0,4 2. P(temperatur=60|Play=no) = 0,060367397 3. P(humidity=high|Play=no) = 4/5 = 0,8 4. P(windy=false|Play=no) = 2/5 = 0,4 Total = 0,007727027","title":"Play = NO"},{"location":"uts/#play-yes","text":"1. P(Outlook=rainy|Play=yes) = 3/9 = 0,333333333 2. P(temperatur=60|Play=yes) = 0,015194057 3. P(humidity=high|Play=yes) = 3/9 = 0,333333333 4. P(windy=false|Play=yes) = 6/9 = 0,666666667 Total = 0,001125486","title":"Play = YES"},{"location":"uts/#hasil","text":"p(X|Ci)*P(Ci) p(X|play = no)*p(no) = 0,357142857 x 0,007727027 = 0,002759652 p(X|play =yes)*p(yes) = 0,642857143 x 0,001125486 = 0,000723527 Jadi Cari kedua hasil ini yang terbesar , yang terbesar adalah Play = NO outlook temperature humidity windy play rainy 60 high FALSE no [Data UTS dalam Bentuk Excel][https://github.com/bimaikhsan/datamining/blob/master/uts.xlsx]","title":"Hasil"}]}